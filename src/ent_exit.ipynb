{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/input/train.csv')\n",
    "entbase = pd.read_csv('../data/input/1entbase.csv')\n",
    "alter = pd.read_csv('../data/input/2alter.csv')\n",
    "branch = pd.read_csv('../data/input/3branch.csv')\n",
    "invest = pd.read_csv('../data/input/4invest.csv')\n",
    "right = pd.read_csv('../data/input/5right.csv')\n",
    "project = pd.read_csv('../data/input/6project.csv')\n",
    "lawsuit = pd.read_csv('../data/input/7lawsuit.csv')\n",
    "breakfaith = pd.read_csv('../data/input/8breakfaith.csv')\n",
    "recruit = pd.read_csv('../data/input/9recruit.csv')\n",
    "qualification = pd.read_csv('../data/input/10qualification.csv', encoding='gbk')\n",
    "test = pd.read_csv('../data/input/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[-2:])\n",
    "    return (year - 2010) * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entbase_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mydf = df.fillna(value={'ZCZB': 0, 'MPNUM': 0, 'INUM': 0, 'ENUM': 0, 'FINZB': 0, 'FSTINUM': 0, 'TZINUM': 0})  # 未处理 HY；ZCZB 为 0 表示缺失或错误\n",
    "    \n",
    "    zczb_gb_prov = mydf.groupby('PROV')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_zczb/sum_gb_prov'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_prov'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_prov'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_prov'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    # bad\n",
    "    zczb_gb_rgyear = mydf.groupby('RGYEAR')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_zczb/sum_gb_rgyear'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_rgyear'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_rgyear'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_rgyear'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_hy = mydf.groupby('HY')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_zczb/sum_gb_hy'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_hy'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_hy'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_hy'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_etype = mydf.groupby('ETYPE')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_zczb/sum_gb_etype'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_etype'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_etype'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_etype'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    ##### bad\n",
    "    mpnum_gb_prov = mydf.groupby('PROV')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_mpnum/sum_gb_prov'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_prov'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_prov'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_prov'] = mydf['MPNUM'] - tmp['median']\n",
    "    \n",
    "    mpnum_gb_rgyear = mydf.groupby('RGYEAR')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_mpnum/sum_gb_rgyear'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_rgyear'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_rgyear'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_rgyear'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_hy = mydf.groupby('HY')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_mpnum/sum_gb_hy'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_hy'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_hy'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_hy'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_etype = mydf.groupby('ETYPE')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_mpnum/sum_gb_etype'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_etype'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_etype'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_etype'] = mydf['MPNUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    inum_gb_prov = mydf.groupby('PROV')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_inum/sum_gb_prov'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_prov'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_prov'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_prov'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_rgyear = mydf.groupby('RGYEAR')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_inum/sum_gb_rgyear'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_rgyear'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_rgyear'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_rgyear'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_hy = mydf.groupby('HY')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_inum/sum_gb_hy'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_hy'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_hy'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_hy'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_etype = mydf.groupby('ETYPE')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_inum/sum_gb_etype'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_etype'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_etype'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_etype'] = mydf['INUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    enum_gb_prov = mydf.groupby('PROV')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_enum/sum_gb_prov'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_prov'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_prov'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_prov'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_rgyear = mydf.groupby('RGYEAR')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_enum/sum_gb_rgyear'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_rgyear'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_rgyear'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_rgyear'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_hy = mydf.groupby('HY')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_enum/sum_gb_hy'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_hy'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_hy'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_hy'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_etype = mydf.groupby('ETYPE')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_enum/sum_gb_etype'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_etype'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_etype'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_etype'] = mydf['ENUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    finzb_gb_prov = mydf.groupby('PROV')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_finzb/sum_gb_prov'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_prov'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_prov'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_prov'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_rgyear = mydf.groupby('RGYEAR')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_finzb/sum_gb_rgyear'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_rgyear'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_rgyear'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_rgyear'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_hy = mydf.groupby('HY')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_finzb/sum_gb_hy'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_hy'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_hy'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_hy'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_etype = mydf.groupby('ETYPE')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_finzb/sum_gb_etype'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_etype'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_etype'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_etype'] = mydf['FINZB'] - tmp['mean']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    fstinum_gb_prov = mydf.groupby('PROV')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_fstinum/sum_gb_prov'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_prov'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_prov'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_prov'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_rgyear = mydf.groupby('RGYEAR')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_fstinum/sum_gb_rgyear'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_rgyear'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_rgyear'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_rgyear'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_hy = mydf.groupby('HY')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_fstinum/sum_gb_hy'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_hy'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_hy'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_hy'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_etype = mydf.groupby('ETYPE')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_fstinum/sum_gb_etype'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_etype'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_etype'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_etype'] = mydf['FSTINUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    tzinum_gb_prov = mydf.groupby('PROV')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_tzinum/sum_gb_prov'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_prov'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_prov'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_prov'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_rgyear = mydf.groupby('RGYEAR')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_tzinum/sum_gb_rgyear'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_rgyear'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_rgyear'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_rgyear'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_hy = mydf.groupby('HY')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_tzinum/sum_gb_hy'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_hy'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_hy'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_hy'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_etype = mydf.groupby('ETYPE')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_tzinum/sum_gb_etype'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_etype'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_etype'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_etype'] = mydf['TZINUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "#     #####\n",
    "#     mydf['mpnum*inum'] = mydf['MPNUM'] * mydf['INUM']\n",
    "#     mydf['mpnum*enum'] = mydf['MPNUM'] * mydf['ENUM']\n",
    "#     mydf['mpnum*fstinum'] = mydf['MPNUM'] * mydf['FSTINUM']\n",
    "#     mydf['mpnum*tzinum'] = mydf['MPNUM'] * mydf['TZINUM']\n",
    "#     mydf['inum*enum'] = mydf['INUM'] * mydf['ENUM']\n",
    "#     mydf['inum*fstinum'] = mydf['INUM'] * mydf['FSTINUM']\n",
    "#     mydf['inum*tzinum'] = mydf['INUM'] * mydf['TZINUM']\n",
    "#     mydf['enum*fstinum'] = mydf['ENUM'] * mydf['FSTINUM']\n",
    "#     mydf['enum*tzinum'] = mydf['ENUM'] * mydf['TZINUM']\n",
    "#     mydf['fstinum*tzinum'] = mydf['FSTINUM'] * mydf['TZINUM']  \n",
    "#     #####\n",
    "    \n",
    "#     ##### bad\n",
    "#     mydf['mpnum*inum*enum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['ENUM']\n",
    "#     mydf['mpnum*inum*fstinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['FSTINUM']\n",
    "#     mydf['mpnum*inum*tzinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['TZINUM']\n",
    "#     mydf['mpnum*enum*fstinum'] = mydf['MPNUM'] * mydf['ENUM'] * mydf['FSTINUM']\n",
    "#     mydf['mpnum*enum*tzinum'] = mydf['MPNUM'] * mydf['ENUM'] * mydf['TZINUM']\n",
    "#     mydf['mpnum*fstinum*tzinum'] = mydf['MPNUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     mydf['inum*enum*fstinum'] = mydf['INUM'] * mydf['ENUM'] * mydf['FSTINUM']\n",
    "#     mydf['inum*enum*tzinum'] = mydf['INUM'] * mydf['ENUM'] * mydf['TZINUM']\n",
    "#     mydf['inum*fstinum*tzinum'] = mydf['INUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     mydf['enum*fstinum*tzinum'] = mydf['ENUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     #####\n",
    "    \n",
    "#     #####\n",
    "#     mydf['mpnum*inum*enum*fstinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['ENUM'] * mydf['FSTINUM']\n",
    "#     mydf['mpnum*inum*enum*tzinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['ENUM'] * mydf['TZINUM']\n",
    "#     mydf['mpnum*inum*fstinum*tzinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     mydf['mpnum*enum*fstinum*tzinum'] = mydf['MPNUM'] * mydf['ENUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     mydf['inum*enum*fstinum*tzinum'] = mydf['INUM'] * mydf['ENUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "    \n",
    "#     mydf['mpnum*inum*enum*fstinum*tzinum'] = mydf['MPNUM'] * mydf['INUM'] * mydf['ENUM'] * mydf['FSTINUM'] * mydf['TZINUM']\n",
    "#     #####\n",
    "    \n",
    "#     #####\n",
    "#     mydf['mpnum+inum+enum+fstinum+tzinum'] = mydf['MPNUM'] + mydf['INUM'] + mydf['ENUM'] + mydf['FSTINUM'] + mydf['TZINUM']\n",
    "    \n",
    "#     mydf['mpnum+inum*enum*fstinum*tzinum'] = mydf['MPNUM'] + mydf['inum*enum*fstinum*tzinum']\n",
    "#     mydf['inum+mpnum*enum*fstinum*tzinum'] = mydf['INUM'] + mydf['mpnum*enum*fstinum*tzinum']\n",
    "#     mydf['enum+mpnum*inum*fstinum*tzinum'] = mydf['ENUM'] + mydf['mpnum*inum*fstinum*tzinum']\n",
    "#     mydf['fstinum+mpnum*inum*enum*tzinum'] = mydf['FSTINUM'] + mydf['mpnum*inum*enum*tzinum']\n",
    "#     mydf['tzinum+mpnum*inum*enum*fstinum'] = mydf['TZINUM'] + mydf['mpnum*inum*enum*fstinum']\n",
    "#     #####\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_alter_feature(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    alt_no = df.groupby(['EID', 'ALTERNO']).size().reset_index()\n",
    "    alt_no = alt_no.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    alt_no.columns = ['EID', 'alt_count', 'alt_types_count']\n",
    "\n",
    "    alt_no_oh = df.groupby(['EID', 'ALTERNO']).size().unstack().reset_index()\n",
    "    alt_no_oh.columns = [i if i == 'EID' else 'alt_' + i for i in alt_no_oh.columns]\n",
    "\n",
    "    df['date'] = df['ALTDATE'].apply(translate_date)\n",
    "    date = df.groupby('EID')['date'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    date.columns = ['EID', 'alt_date_min', 'alt_date_max', 'alt_date_ptp', 'alt_date_std']\n",
    "\n",
    "    df['altbe'] = df['ALTBE'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    df['altaf'] = df['ALTAF'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    alt_be_af = df.groupby('EID')['altbe', 'altaf'].agg([min, max, np.mean]).reset_index()\n",
    "    alt_be_af.columns = ['EID', 'alt_be_min', 'alt_be_max', 'alt_be_mean', 'alt_af_min', 'alt_af_max', 'alt_af_mean']\n",
    "\n",
    "    mydf = pd.merge(alt_no, alt_no_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_right_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rig_type = df.groupby(['EID', 'RIGHTTYPE']).size().reset_index()\n",
    "    rig_type = rig_type.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rig_type.columns = ['EID', 'rig_count', 'rig_types_count']\n",
    "    \n",
    "    rig_type_oh_rate = df.groupby(['EID', 'RIGHTTYPE']).size().unstack().reset_index()\n",
    "    rig_type_oh_rate.iloc[:, 1:] = rig_type_oh_rate.iloc[:, 1:].div(rig_type['rig_count'], axis='index')\n",
    "    rig_type_oh_rate.columns = [i if i == 'EID' else 'rig_rate_' + str(i) for i in rig_type_oh_rate.columns]\n",
    "    \n",
    "    df['ask_month'] = (pd.to_datetime(df['ASKDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    ask_date = df.groupby('EID')['ask_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    ask_date.columns = ['EID', 'rig_askdate_max', 'rig_askdate_min', 'rig_askdate_ptp', 'rig_askdate_std']\n",
    "\n",
    "    df['get_month'] = (pd.to_datetime(df['FBDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    get_date = df.groupby('EID')['get_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    get_date.columns = ['EID', 'rig_getdate_max', 'rig_getdate_min', 'rig_getdate_ptp', 'rig_getdate_std']\n",
    "    \n",
    "    # bad\n",
    "    unget = df[df.FBDATE.isnull()]\n",
    "    unget = unget.groupby('EID').size().reset_index()\n",
    "    unget.columns = ['EID', 'rig_unget_num']\n",
    "    \n",
    "    right_1year = df[df['ASKDATE'] >= '2015-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_1year.columns = ['EID', 'ask_num(1year)']\n",
    "    right_2year = df[df['ASKDATE'] >= '2014-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_2year.columns = ['EID', 'ask_num(2year)']\n",
    "    right_5year = df[df['ASKDATE'] >= '2010-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_5year.columns = ['EID', 'ask_num(5year)']\n",
    "    right_end_1year = df[df['FBDATE'] >= '2015-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_1year.columns = ['EID', 'get_num(1year)']\n",
    "    right_end_2year = df[df['FBDATE'] >= '2014-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_2year.columns = ['EID', 'get_num(2year)']\n",
    "    right_end_5year = df[df['FBDATE'] >= '2010-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_5year.columns = ['EID', 'get_num(5year)']\n",
    "    \n",
    "    mydf = pd.merge(rig_type, rig_type_oh_rate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, get_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, unget, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_5year, how='left', on='EID')\n",
    "    \n",
    "    # bad\n",
    "    mydf['ask_rate(1year)'] = mydf['ask_num(1year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(2year)'] = mydf['ask_num(2year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(5year)'] = mydf['ask_num(5year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(1year)'] = mydf['get_num(1year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(2year)'] = mydf['get_num(2year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(5year)'] = mydf['get_num(5year)'] / mydf['rig_count']\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recruit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rec_wz = df.groupby(['EID', 'WZCODE']).size().reset_index()\n",
    "    rec_wz = rec_wz.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_wz.columns = ['EID', 'rec_wz_count', 'rec_wz_types_count']\n",
    "    \n",
    "    # bad\n",
    "    rec_wz_oh = df.groupby(['EID', 'WZCODE']).size().unstack().reset_index()\n",
    "    rec_wz_oh.columns = [i if i == 'EID' else 'rec_wz_' + i for i in rec_wz_oh.columns]\n",
    "    \n",
    "    # bad\n",
    "    rec_pos = df.groupby(['EID', 'POSCODE']).size().reset_index()\n",
    "    rec_pos = rec_pos.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_pos.columns = ['EID', 'rec_pos_count', 'rec_pos_types_count']\n",
    "    \n",
    "    df['recdate'] = (pd.to_datetime(df['RECDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    rec_date = df.groupby('EID')['recdate'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_date.columns = ['EID', 'rec_date_max', 'rec_date_min', 'rec_date_ptp', 'rec_date_std']\n",
    "    \n",
    "    # bad\n",
    "    df['pnum'] = df['PNUM'].str.extract('(\\d+)').fillna(1).astype(int)  # 若干=1\n",
    "    rec_num = df.groupby('EID')['pnum'].agg([sum, max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_num.columns = ['EID' if i == 'EID' else 'rec_num_' + i for i in rec_num.columns]\n",
    "    \n",
    "    mydf = pd.merge(rec_wz, rec_wz_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_pos, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branch_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bra_num = df.groupby('EID')['TYPECODE'].size().reset_index()\n",
    "    bra_num.columns = ['EID', 'bra_count']\n",
    "    \n",
    "    # bad\n",
    "    bra_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    bra_home.columns = ['EID', 'bra_nothome', 'bra_home']\n",
    "    \n",
    "    bra_year = df.groupby('EID')['B_REYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_year.columns = [i if i == 'EID' else 'bra_year_' + i for i in bra_year.columns]\n",
    "    \n",
    "    bra_endyear = df.groupby('EID')['B_ENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_endyear.columns = [i if i == 'EID' else 'bra_endyear_' + i for i in bra_endyear.columns]\n",
    "    \n",
    "    bra_end_num = df[~df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_end_num.columns = ['EID', 'bra_end_num']\n",
    "    bra_notend_num = df[df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_notend_num.columns = ['EID', 'bra_notend_num']\n",
    "    \n",
    "    mydf = pd.merge(bra_num, bra_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_end_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_notend_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_invest_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    inv_num = df.groupby('EID').size().reset_index()\n",
    "    inv_num.columns = ['EID', 'inv_count']\n",
    "    \n",
    "    # bad\n",
    "    inv_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inv_home.columns = ['EID', 'inv_nothome_num', 'inv_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inv_bl = df.groupby('EID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_bl.columns = [i if i == 'EID' else 'inv_bl_' + i for i in inv_bl.columns]\n",
    "    \n",
    "    inv_year = df.groupby('EID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_year.columns = [i if i == 'EID' else 'inv_year_' + i for i in inv_year.columns]\n",
    "    \n",
    "    # bad\n",
    "    inv_endyear = df.groupby('EID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_endyear.columns = [i if i == 'EID' else 'inv_endyear_' + i for i in inv_endyear.columns]\n",
    "    \n",
    "    # bad\n",
    "    inved_num = df.groupby('BTEID').size().reset_index()\n",
    "    inved_num.columns = ['EID', 'inved_num']\n",
    "    \n",
    "    inved_home = df.groupby(['BTEID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inved_home.columns = ['EID', 'inved_nothome_num', 'inved_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inved_bl = df.groupby('BTEID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_bl.columns = ['EID' if i == 'BTEID' else 'inved_bl_' + i for i in inved_bl.columns]\n",
    "    \n",
    "    inved_year = df.groupby('BTEID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_year.columns = ['EID' if i == 'BTEID' else 'inved_year_' + i for i in inved_year.columns]\n",
    "    \n",
    "    inved_endyear = df.groupby('BTEID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_endyear.columns = ['EID' if i == 'BTEID' else 'inved_endyear_' + i for i in inved_endyear.columns]\n",
    "    \n",
    "    mydf = pd.merge(inv_num, inv_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_endyear, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lawsuit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    law_num = df.groupby('EID').size().reset_index()\n",
    "    law_num.columns = ['EID', 'law_count']\n",
    "    \n",
    "    # bad\n",
    "    df['lawdate'] = df['LAWDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    law_date = df.groupby('EID')['lawdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    law_date.columns = [i if i == 'EID' else 'law_date_' + i for i in law_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    law_amout = df.groupby('EID')['LAWAMOUNT'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    law_amout.columns = [i if i == 'EID' else 'law_amout_' + i for i in law_amout.columns]\n",
    "    \n",
    "    mydf = pd.merge(law_num, law_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, law_amout, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_project_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    pro_num = df.groupby('EID').size().reset_index()\n",
    "    pro_num.columns = ['EID', 'pro_count']\n",
    "    \n",
    "    df['djdate'] = df['DJDATE'].apply(translate_date)\n",
    "    pro_date = df.groupby('EID')['djdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    pro_date.columns = [i if i == 'EID' else 'pro_date_' + i for i in pro_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    pro_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    pro_home.columns = ['EID', 'pro_nothome_num', 'pro_home_num']\n",
    "    \n",
    "    mydf = pd.merge(pro_num, pro_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, pro_home, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qualification_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    qua_num = df.groupby('EID').size().reset_index()\n",
    "    qua_num.columns = ['EID', 'qua_count']\n",
    "    \n",
    "    # bad\n",
    "    qua_type = df.groupby(['EID', 'ADDTYPE']).size().unstack().reset_index()\n",
    "    qua_type.columns = [i if i == 'EID' else 'qua_type_' + str(i) for i in qua_type.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['begindate'] = df['BEGINDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '')).apply(translate_date)\n",
    "    qua_begindate = df.groupby('EID')['begindate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_begindate.columns = [i if i == 'EID' else 'qua_begindate_' + i for i in qua_begindate.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['expirydate'] = df['EXPIRYDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '') if not pd.isnull(x) else np.nan)\n",
    "    df['expirydate'] = (pd.to_datetime(df['expirydate']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    qua_expirydate = df.groupby('EID')['expirydate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_expirydate.columns = [i if i == 'EID' else 'qua_expirydate_' + i for i in qua_expirydate.columns]\n",
    "    \n",
    "    mydf = pd.merge(qua_num, qua_type, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begindate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_expirydate, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_breakfaith_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bre_num = df.groupby('EID').size().reset_index()\n",
    "    bre_num.columns = ['EID', 'bre_count']\n",
    "    \n",
    "    # bad\n",
    "    df['fbdate'] = df['FBDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    bre_date = df.groupby('EID')['fbdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_date.columns = [i if i == 'EID' else 'bre_date_' + i for i in bre_date.columns]\n",
    "    \n",
    "    df['sxenddate'] = (pd.to_datetime(df['SXENDDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    bre_enddate = df.groupby('EID')['sxenddate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_enddate.columns = [i if i == 'EID' else 'bre_enddate_' + i for i in bre_enddate.columns]\n",
    "    \n",
    "    mydf = pd.merge(bre_num, bre_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_enddate, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "entbase_feat = get_entbase_feature(entbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:16: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "alter_feat = get_alter_feature(alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_feature = get_right_feature(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:22: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "recruit_feat = get_recruit_feature(recruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_feat = get_branch_feature(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_feat = get_invest_feature(invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lawsuit_feat = get_lawsuit_feature(lawsuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_feat = get_project_feature(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualification_feat = get_qualification_feature(qualification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfaith_feat = get_breakfaith_feature(breakfaith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, right_feature, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, recruit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, branch_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, invest_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, lawsuit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, project_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, qualification_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, breakfaith_feat, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = pd.merge(train, dataset, on='EID', how='left')\n",
    "testset = pd.merge(test, dataset, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218264, 278) (218264L,) (218247, 278)\n"
     ]
    }
   ],
   "source": [
    "train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)\n",
    "train_label = trainset.TARGET.values\n",
    "test_feature = testset\n",
    "test_index = testset.EID.values\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# EID 前面的字母代表不同省份，已提供了 PROV 列，因此字母是冗余信息，直接舍弃\n",
    "train_feature['EID'] = train_feature['EID'].str.extract('(\\d+)').astype(int)\n",
    "test_feature['EID'] = test_feature['EID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.concat([train_feature, trainset.TARGET], axis=1).to_csv('../data/output/feat/train_xxy_local6864_online6923.csv', index=False)\n",
    "# test_feature.to_csv('../data/output/feat/test_xxy_local6864_online6923.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 3\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "#     'objective': 'rank:pairwise',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights ': 0,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 20,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, params, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = xgb.cv(params, dtrain, num_round, verbose_eval=10, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res), res.loc[len(res) - 1, 'test-auc-mean']\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=50)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, threshold, name):\n",
    "    result = pd.DataFrame({'EID': test_index, 'FORTARGET': 0, 'PROB': pred})\n",
    "    mask = result['PROB'] >= threshold\n",
    "    result.at[mask, 'FORTARGET'] = 1\n",
    "    # result['PROB'] = result['PROB'].apply(lambda x: round(x, 4))\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'EID', u'PROV', u'RGYEAR', u'HY', u'ZCZB', u'ETYPE', u'MPNUM', u'INUM',\n",
      "       u'ENUM', u'FINZB',\n",
      "       ...\n",
      "       u'qua_expirydate_std', u'bre_count', u'bre_date_min', u'bre_date_max',\n",
      "       u'bre_date_ptp', u'bre_date_std', u'bre_enddate_min',\n",
      "       u'bre_enddate_max', u'bre_enddate_ptp', u'bre_enddate_std'],\n",
      "      dtype='object', length=310)\n",
      "run cv: round: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "[0]\tcv-test-auc:0.631895666667+0.00709779826582\tcv-train-auc:0.652329333333+0.00471109303194\n",
      "[10]\tcv-test-auc:0.663725+0.00239870395561\tcv-train-auc:0.703206+0.00420763187553\n",
      "[20]\tcv-test-auc:0.666320333333+0.0034847747257\tcv-train-auc:0.709755+0.00233518193438\n",
      "[30]\tcv-test-auc:0.668016666667+0.00302323935907\tcv-train-auc:0.713953666667+0.00225188518555\n",
      "[40]\tcv-test-auc:0.669386+0.00293171212775\tcv-train-auc:0.717448+0.00141758385995\n",
      "[50]\tcv-test-auc:0.670225333333+0.00302096864525\tcv-train-auc:0.720183+0.00100285326278\n",
      "[60]\tcv-test-auc:0.671043333333+0.00281076458558\tcv-train-auc:0.723117+0.000738941585422\n",
      "[70]\tcv-test-auc:0.671638333333+0.00300688325162\tcv-train-auc:0.725617333333+0.000604099513509\n",
      "[80]\tcv-test-auc:0.672291666667+0.00311884875484\tcv-train-auc:0.728247+0.000942299669249\n",
      "[90]\tcv-test-auc:0.672901333333+0.00294634828597\tcv-train-auc:0.730574333333+0.00104526115822\n",
      "[100]\tcv-test-auc:0.673334666667+0.00278652427866\tcv-train-auc:0.732710666667+0.0010838478778\n",
      "[110]\tcv-test-auc:0.673888666667+0.00289212912728\tcv-train-auc:0.734940666667+0.00101452988566\n",
      "[120]\tcv-test-auc:0.674340666667+0.00285609084045\tcv-train-auc:0.737278333333+0.00110280742753\n",
      "[130]\tcv-test-auc:0.674905666667+0.00263633891768\tcv-train-auc:0.739277666667+0.00105059866531\n",
      "[140]\tcv-test-auc:0.675435+0.00246685927176\tcv-train-auc:0.741254666667+0.00109502278008\n",
      "[150]\tcv-test-auc:0.675997+0.00237829953258\tcv-train-auc:0.743112666667+0.00119597723873\n",
      "[160]\tcv-test-auc:0.676472666667+0.00246304342002\tcv-train-auc:0.745029+0.00110524778519\n",
      "[170]\tcv-test-auc:0.676994666667+0.00248800513576\tcv-train-auc:0.746873666667+0.00107939623041\n",
      "[180]\tcv-test-auc:0.677402333333+0.0024516756628\tcv-train-auc:0.748836666667+0.00101697995173\n",
      "[190]\tcv-test-auc:0.677866333333+0.00244719176926\tcv-train-auc:0.750724333333+0.00109707044239\n",
      "[200]\tcv-test-auc:0.678219+0.00241033151247\tcv-train-auc:0.752513+0.00106987880934\n",
      "[210]\tcv-test-auc:0.678593666667+0.00244909538855\tcv-train-auc:0.754171666667+0.0010587367735\n",
      "[220]\tcv-test-auc:0.679030333333+0.00250414008305\tcv-train-auc:0.755837+0.00127184459218\n",
      "[230]\tcv-test-auc:0.679412666667+0.00246027780726\tcv-train-auc:0.757521333333+0.00126727564308\n",
      "[240]\tcv-test-auc:0.679784333333+0.00246443128441\tcv-train-auc:0.759136333333+0.00120344431621\n",
      "[250]\tcv-test-auc:0.680211+0.00245632666123\tcv-train-auc:0.760635666667+0.00114657237984\n",
      "[260]\tcv-test-auc:0.680593333333+0.00242605912725\tcv-train-auc:0.761963666667+0.00107574263754\n",
      "[270]\tcv-test-auc:0.680886333333+0.0024074459957\tcv-train-auc:0.763457+0.00106444727441\n",
      "[280]\tcv-test-auc:0.681146666667+0.00237051349336\tcv-train-auc:0.764846+0.000972611947284\n",
      "[290]\tcv-test-auc:0.681405333333+0.00232602125719\tcv-train-auc:0.7662+0.00114984955538\n",
      "[300]\tcv-test-auc:0.681625666667+0.00235765000701\tcv-train-auc:0.767528+0.00120287350402\n",
      "[310]\tcv-test-auc:0.681910666667+0.00232993652751\tcv-train-auc:0.768827666667+0.00106645029055\n",
      "[320]\tcv-test-auc:0.682187666667+0.00232260691657\tcv-train-auc:0.770151+0.00117946428517\n",
      "[330]\tcv-test-auc:0.682377333333+0.00235041079152\tcv-train-auc:0.771310333333+0.00103262588686\n",
      "[340]\tcv-test-auc:0.682666333333+0.00231490263774\tcv-train-auc:0.772567+0.000939756351402\n",
      "[350]\tcv-test-auc:0.682819666667+0.00230417669076\tcv-train-auc:0.773828+0.00106324628693\n",
      "[360]\tcv-test-auc:0.683002+0.00237151695475\tcv-train-auc:0.774954333333+0.00100011910402\n",
      "[370]\tcv-test-auc:0.683201333333+0.00243257358002\tcv-train-auc:0.77598+0.00101054077932\n",
      "[380]\tcv-test-auc:0.683373333333+0.00241484027537\tcv-train-auc:0.777153333333+0.000969939631569\n",
      "[390]\tcv-test-auc:0.683530666667+0.00243059832597\tcv-train-auc:0.778129666667+0.00102682206616\n",
      "[400]\tcv-test-auc:0.683702+0.00240706557174\tcv-train-auc:0.779251333333+0.00105650282641\n",
      "[410]\tcv-test-auc:0.683836666667+0.00240581134386\tcv-train-auc:0.780173666667+0.00113848739807\n",
      "[420]\tcv-test-auc:0.684013333333+0.00240231073945\tcv-train-auc:0.781237+0.00111238212859\n",
      "[430]\tcv-test-auc:0.684147333333+0.00238533314701\tcv-train-auc:0.782318333333+0.00107787022513\n",
      "[440]\tcv-test-auc:0.684281666667+0.00236496065271\tcv-train-auc:0.783454333333+0.00103553217666\n",
      "[450]\tcv-test-auc:0.684352333333+0.00236214229508\tcv-train-auc:0.784361666667+0.00103253259297\n",
      "[460]\tcv-test-auc:0.684472333333+0.00239596832663\tcv-train-auc:0.785345666667+0.00121765685734\n",
      "[470]\tcv-test-auc:0.684550333333+0.00242364702234\tcv-train-auc:0.786198666667+0.00110159167672\n",
      "[480]\tcv-test-auc:0.684645+0.00244914801513\tcv-train-auc:0.787147666667+0.00122311769217\n",
      "[490]\tcv-test-auc:0.684746666667+0.00241320980347\tcv-train-auc:0.788113+0.00117337575681\n",
      "[500]\tcv-test-auc:0.684836333333+0.00240339015189\tcv-train-auc:0.788874333333+0.00109918222127\n",
      "[510]\tcv-test-auc:0.684924+0.0023792318088\tcv-train-auc:0.789773+0.00111915712331\n",
      "[520]\tcv-test-auc:0.684996+0.00234292822482\tcv-train-auc:0.790648666667+0.00118724227613\n",
      "[530]\tcv-test-auc:0.685091666667+0.00235262723118\tcv-train-auc:0.791665666667+0.00132794235149\n",
      "[540]\tcv-test-auc:0.685163+0.00238879648917\tcv-train-auc:0.792418666667+0.00127200218379\n",
      "[550]\tcv-test-auc:0.685245333333+0.0023850445605\tcv-train-auc:0.793379666667+0.00123443383334\n",
      "[560]\tcv-test-auc:0.685332333333+0.00243305381956\tcv-train-auc:0.794281333333+0.00120373594373\n",
      "[570]\tcv-test-auc:0.685409+0.00244478887977\tcv-train-auc:0.795214333333+0.00140078128993\n",
      "[580]\tcv-test-auc:0.685433666667+0.00247230342438\tcv-train-auc:0.796054+0.00140171490206\n",
      "[590]\tcv-test-auc:0.685497+0.00246337018466\tcv-train-auc:0.796848+0.00132497471674\n",
      "[600]\tcv-test-auc:0.685546666667+0.00243235226798\tcv-train-auc:0.797614+0.00124924697318\n",
      "[610]\tcv-test-auc:0.685557666667+0.00243113421175\tcv-train-auc:0.798408666667+0.00118970509885\n",
      "[620]\tcv-test-auc:0.685565333333+0.0024331643777\tcv-train-auc:0.799300666667+0.00135025289812\n",
      "[630]\tcv-test-auc:0.685594+0.00245890883659\tcv-train-auc:0.800174333333+0.0013756725224\n",
      "[640]\tcv-test-auc:0.68563+0.00246214554133\tcv-train-auc:0.801025333333+0.00139662983245\n",
      "[650]\tcv-test-auc:0.685690333333+0.00243188066228\tcv-train-auc:0.801885+0.00142607246193\n",
      "[660]\tcv-test-auc:0.685753333333+0.00241571374316\tcv-train-auc:0.802801+0.00153342753334\n",
      "[670]\tcv-test-auc:0.685800333333+0.00239883337386\tcv-train-auc:0.803533666667+0.00153423864579\n",
      "[680]\tcv-test-auc:0.685811+0.00238371600657\tcv-train-auc:0.804468333333+0.00158564694123\n",
      "[690]\tcv-test-auc:0.685877333333+0.00239467204899\tcv-train-auc:0.805323+0.00173703732449\n",
      "[700]\tcv-test-auc:0.685964+0.002377575656\tcv-train-auc:0.806221333333+0.00175424520014\n",
      "[710]\tcv-test-auc:0.685982666667+0.00234551264238\tcv-train-auc:0.807053+0.00183647179849\n",
      "[720]\tcv-test-auc:0.686029+0.00237011490579\tcv-train-auc:0.807876333333+0.00177013640403\n",
      "[730]\tcv-test-auc:0.686066+0.00235599886814\tcv-train-auc:0.808808666667+0.00181859603968\n",
      "[740]\tcv-test-auc:0.686084666667+0.0023369463456\tcv-train-auc:0.809747+0.00187142352235\n",
      "[750]\tcv-test-auc:0.686117333333+0.00232702919812\tcv-train-auc:0.810485666667+0.00186670874953\n",
      "[760]\tcv-test-auc:0.686154+0.00229010174446\tcv-train-auc:0.811163+0.00191487092689\n",
      "[770]\tcv-test-auc:0.686207+0.00229074762177\tcv-train-auc:0.812097666667+0.00186826949043\n",
      "[780]\tcv-test-auc:0.686212+0.00233106499266\tcv-train-auc:0.812977+0.00170399374021\n",
      "[790]\tcv-test-auc:0.686245+0.0023060147441\tcv-train-auc:0.813644666667+0.00166328837615\n",
      "[800]\tcv-test-auc:0.686246+0.00228275403844\tcv-train-auc:0.814535666667+0.00157358705581\n",
      "[810]\tcv-test-auc:0.686241333333+0.00225827726868\tcv-train-auc:0.815361666667+0.00156813526911\n",
      "[820]\tcv-test-auc:0.686248+0.0022829599208\tcv-train-auc:0.816075333333+0.00152143229301\n",
      "[830]\tcv-test-auc:0.686294+0.00226032033128\tcv-train-auc:0.816985+0.0016773145203\n",
      "[840]\tcv-test-auc:0.686306666667+0.00226723463472\tcv-train-auc:0.817750666667+0.00172440256965\n",
      "[850]\tcv-test-auc:0.686310666667+0.0022394627828\tcv-train-auc:0.818612333333+0.00188697188344\n",
      "[860]\tcv-test-auc:0.686328666667+0.00223440317659\tcv-train-auc:0.819284333333+0.00185809227136\n",
      "[870]\tcv-test-auc:0.686334666667+0.00222224096103\tcv-train-auc:0.819965333333+0.00180697838639\n",
      "[880]\tcv-test-auc:0.686335333333+0.00222285017239\tcv-train-auc:0.820777666667+0.00176943537762\n",
      "[890]\tcv-test-auc:0.686345+0.00223346651344\tcv-train-auc:0.821492333333+0.00189724033504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[900]\tcv-test-auc:0.686389333333+0.00222767147987\tcv-train-auc:0.822189+0.0019811920654\n",
      "[910]\tcv-test-auc:0.686403666667+0.0022225994291\tcv-train-auc:0.822892666667+0.00192095554926\n",
      "[920]\tcv-test-auc:0.686399+0.00221135117066\tcv-train-auc:0.823614+0.00191880031964\n",
      "[930]\tcv-test-auc:0.686411+0.00219476893241\tcv-train-auc:0.824349666667+0.00189730727319\n",
      "[940]\tcv-test-auc:0.686413666667+0.00218879210728\tcv-train-auc:0.825183666667+0.00202109249885\n",
      "[950]\tcv-test-auc:0.686439333333+0.00218582011052\tcv-train-auc:0.825943333333+0.00209528619101\n",
      "[960]\tcv-test-auc:0.686436333333+0.00219058079564\tcv-train-auc:0.826722666667+0.00213803092172\n",
      "[970]\tcv-test-auc:0.686438+0.00216758406219\tcv-train-auc:0.827564333333+0.00201560848271\n",
      "[980]\tcv-test-auc:0.686458666667+0.00217154082521\tcv-train-auc:0.828281666667+0.00202617378875\n",
      "[990]\tcv-test-auc:0.686469+0.00216303305569\tcv-train-auc:0.828974+0.00200574491565\n",
      "[1000]\tcv-test-auc:0.686467333333+0.00218605494492\tcv-train-auc:0.829660666667+0.0019365951794\n",
      "[1010]\tcv-test-auc:0.686461666667+0.002193458811\tcv-train-auc:0.830371333333+0.00181242753112\n",
      "[1020]\tcv-test-auc:0.686448666667+0.00220786266682\tcv-train-auc:0.831098333333+0.00179918913835\n",
      "[1030]\tcv-test-auc:0.686463333333+0.00223480205437\tcv-train-auc:0.831795+0.00175847831946\n",
      "[1040]\tcv-test-auc:0.686445333333+0.00222241345288\tcv-train-auc:0.832544333333+0.00166832937062\n",
      "[1050]\tcv-test-auc:0.68646+0.00222874000278\tcv-train-auc:0.833218333333+0.00170906296614\n",
      "[1060]\tcv-test-auc:0.686474333333+0.00227831375266\tcv-train-auc:0.833921666667+0.00177343176419\n",
      "[1070]\tcv-test-auc:0.686477333333+0.00224591278153\tcv-train-auc:0.834606+0.00176503389958\n",
      "[1080]\tcv-test-auc:0.686458666667+0.00222679026603\tcv-train-auc:0.835352333333+0.00171303810686\n",
      "[1090]\tcv-test-auc:0.686485+0.00224820165169\tcv-train-auc:0.835977+0.00167785656916\n",
      "[1100]\tcv-test-auc:0.686485666667+0.00224225456975\tcv-train-auc:0.836500666667+0.00166733826469\n",
      "[1110]\tcv-test-auc:0.686484666667+0.0022167930791\tcv-train-auc:0.837215666667+0.00170012594305\n",
      "[1120]\tcv-test-auc:0.68647+0.00222093809009\tcv-train-auc:0.837954+0.00166790007694\n",
      "[1130]\tcv-test-auc:0.686475+0.00220919397066\tcv-train-auc:0.838619666667+0.00175158410843\n",
      "[1140]\tcv-test-auc:0.686483+0.00220745418979\tcv-train-auc:0.839329+0.00163205902671\n",
      "[1150]\tcv-test-auc:0.686492+0.00219376814332\tcv-train-auc:0.840061+0.00152223541762\n",
      "[1160]\tcv-test-auc:0.686472666667+0.00218552119388\tcv-train-auc:0.840885666667+0.00144667304146\n",
      "[1170]\tcv-test-auc:0.686472333333+0.00218413832488\tcv-train-auc:0.841493666667+0.00135901369464\n",
      "[1180]\tcv-test-auc:0.686498666667+0.00215808284261\tcv-train-auc:0.842054+0.00130229054618\n",
      "[1190]\tcv-test-auc:0.686489666667+0.00213759059587\tcv-train-auc:0.842656333333+0.00132087883202\n",
      "[1200]\tcv-test-auc:0.686456333333+0.00214212392006\tcv-train-auc:0.843274333333+0.00115945974009\n",
      "[1210]\tcv-test-auc:0.686431+0.00214049729424\tcv-train-auc:0.843934666667+0.00109862712307\n",
      "[1220]\tcv-test-auc:0.686427333333+0.0021412626078\tcv-train-auc:0.844564+0.00111098424831\n",
      "[1230]\tcv-test-auc:0.686433+0.00215233144907\tcv-train-auc:0.845114666667+0.00108407789798\n",
      "[1240]\tcv-test-auc:0.686431666667+0.00213641168525\tcv-train-auc:0.845739+0.000962622459742\n",
      "[1250]\tcv-test-auc:0.686395+0.0021318725728\tcv-train-auc:0.846329+0.000989279872769\n",
      "[1260]\tcv-test-auc:0.686419666667+0.00210506014694\tcv-train-auc:0.846944666667+0.000925087503369\n",
      "[1270]\tcv-test-auc:0.686427666667+0.00208681421204\tcv-train-auc:0.847686666667+0.000943821075322\n",
      "[1280]\tcv-test-auc:0.686429666667+0.00211036431821\tcv-train-auc:0.848417+0.00101245180955\n",
      "Stopping. Best iteration:\n",
      "[1181] cv-mean:0.686501333333\tcv-std:0.00215312878285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 1871.1815087 s\n"
     ]
    }
   ],
   "source": [
    "iterations, best_score = xgb_cv(train_feature, train_label, params, config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.660541\n",
      "[50]\ttrain-auc:0.711973\n",
      "[100]\ttrain-auc:0.721101\n",
      "[150]\ttrain-auc:0.729243\n",
      "[200]\ttrain-auc:0.736644\n",
      "[250]\ttrain-auc:0.743824\n",
      "[300]\ttrain-auc:0.749417\n",
      "[350]\ttrain-auc:0.755179\n",
      "[400]\ttrain-auc:0.759895\n",
      "[450]\ttrain-auc:0.764829\n",
      "[500]\ttrain-auc:0.769332\n",
      "[550]\ttrain-auc:0.773301\n",
      "[600]\ttrain-auc:0.777194\n",
      "[650]\ttrain-auc:0.780988\n",
      "[700]\ttrain-auc:0.784439\n",
      "[750]\ttrain-auc:0.788210\n",
      "[800]\ttrain-auc:0.791807\n",
      "[850]\ttrain-auc:0.795155\n",
      "[900]\ttrain-auc:0.798286\n",
      "[950]\ttrain-auc:0.801571\n",
      "[1000]\ttrain-auc:0.804901\n",
      "[1050]\ttrain-auc:0.808207\n",
      "[1100]\ttrain-auc:0.811005\n",
      "[1150]\ttrain-auc:0.814199\n",
      "[1181]\ttrain-auc:0.815727\n"
     ]
    }
   ],
   "source": [
    "model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(model.get_fscore().items(), columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "importance.to_csv('../data/output/feat_imp/importance-1128-%f.csv' % best_score, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, 0.21, '1128-xgb-%f' % best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
