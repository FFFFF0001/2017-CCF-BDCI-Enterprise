{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/input/train.csv')\n",
    "entbase = pd.read_csv('../data/input/1entbase.csv')\n",
    "alter = pd.read_csv('../data/input/2alter.csv')\n",
    "branch = pd.read_csv('../data/input/3branch.csv')\n",
    "invest = pd.read_csv('../data/input/4invest.csv')\n",
    "right = pd.read_csv('../data/input/5right.csv')\n",
    "project = pd.read_csv('../data/input/6project.csv')\n",
    "lawsuit = pd.read_csv('../data/input/7lawsuit.csv')\n",
    "breakfaith = pd.read_csv('../data/input/8breakfaith.csv')\n",
    "recruit = pd.read_csv('../data/input/9recruit.csv')\n",
    "qualification = pd.read_csv('../data/input/10qualification.csv', encoding='gbk')\n",
    "test = pd.read_csv('../data/input/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[-2:])\n",
    "    return (year - 2010) * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entbase_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mydf = df.fillna(value={'ZCZB': 0, 'MPNUM': 0, 'INUM': 0, 'ENUM': 0, 'FINZB': 0, 'FSTINUM': 0, 'TZINUM': 0})  # 未处理 HY；ZCZB 为 0 表示缺失或错误\n",
    "    \n",
    "    zczb_gb_prov = mydf.groupby('PROV')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_zczb/sum_gb_prov'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_prov'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_prov'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_prov'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    # bad\n",
    "    zczb_gb_rgyear = mydf.groupby('RGYEAR')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_zczb/sum_gb_rgyear'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_rgyear'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_rgyear'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_rgyear'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_hy = mydf.groupby('HY')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_zczb/sum_gb_hy'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_hy'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_hy'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_hy'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_etype = mydf.groupby('ETYPE')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_zczb/sum_gb_etype'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_etype'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_etype'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_etype'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    ##### bad\n",
    "    mpnum_gb_prov = mydf.groupby('PROV')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_mpnum/sum_gb_prov'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_prov'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_prov'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_prov'] = mydf['MPNUM'] - tmp['median']\n",
    "    \n",
    "    mpnum_gb_rgyear = mydf.groupby('RGYEAR')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_mpnum/sum_gb_rgyear'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_rgyear'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_rgyear'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_rgyear'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_hy = mydf.groupby('HY')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_mpnum/sum_gb_hy'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_hy'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_hy'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_hy'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_etype = mydf.groupby('ETYPE')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_mpnum/sum_gb_etype'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_etype'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_etype'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_etype'] = mydf['MPNUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    inum_gb_prov = mydf.groupby('PROV')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_inum/sum_gb_prov'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_prov'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_prov'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_prov'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_rgyear = mydf.groupby('RGYEAR')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_inum/sum_gb_rgyear'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_rgyear'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_rgyear'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_rgyear'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_hy = mydf.groupby('HY')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_inum/sum_gb_hy'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_hy'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_hy'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_hy'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_etype = mydf.groupby('ETYPE')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_inum/sum_gb_etype'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_etype'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_etype'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_etype'] = mydf['INUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    enum_gb_prov = mydf.groupby('PROV')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_enum/sum_gb_prov'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_prov'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_prov'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_prov'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_rgyear = mydf.groupby('RGYEAR')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_enum/sum_gb_rgyear'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_rgyear'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_rgyear'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_rgyear'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_hy = mydf.groupby('HY')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_enum/sum_gb_hy'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_hy'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_hy'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_hy'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_etype = mydf.groupby('ETYPE')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_enum/sum_gb_etype'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_etype'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_etype'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_etype'] = mydf['ENUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    finzb_gb_prov = mydf.groupby('PROV')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_finzb/sum_gb_prov'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_prov'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_prov'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_prov'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_rgyear = mydf.groupby('RGYEAR')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_finzb/sum_gb_rgyear'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_rgyear'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_rgyear'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_rgyear'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_hy = mydf.groupby('HY')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_finzb/sum_gb_hy'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_hy'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_hy'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_hy'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_etype = mydf.groupby('ETYPE')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_finzb/sum_gb_etype'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_etype'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_etype'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_etype'] = mydf['FINZB'] - tmp['mean']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    fstinum_gb_prov = mydf.groupby('PROV')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_fstinum/sum_gb_prov'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_prov'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_prov'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_prov'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_rgyear = mydf.groupby('RGYEAR')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_fstinum/sum_gb_rgyear'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_rgyear'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_rgyear'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_rgyear'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_hy = mydf.groupby('HY')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_fstinum/sum_gb_hy'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_hy'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_hy'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_hy'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_etype = mydf.groupby('ETYPE')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_fstinum/sum_gb_etype'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_etype'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_etype'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_etype'] = mydf['FSTINUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    tzinum_gb_prov = mydf.groupby('PROV')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_tzinum/sum_gb_prov'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_prov'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_prov'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_prov'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_rgyear = mydf.groupby('RGYEAR')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_tzinum/sum_gb_rgyear'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_rgyear'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_rgyear'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_rgyear'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_hy = mydf.groupby('HY')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_tzinum/sum_gb_hy'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_hy'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_hy'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_hy'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_etype = mydf.groupby('ETYPE')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_tzinum/sum_gb_etype'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_etype'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_etype'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_etype'] = mydf['TZINUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_alter_feature(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    alt_no = df.groupby(['EID', 'ALTERNO']).size().reset_index()\n",
    "    alt_no = alt_no.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    alt_no.columns = ['EID', 'alt_count', 'alt_types_count']\n",
    "\n",
    "    alt_no_oh = df.groupby(['EID', 'ALTERNO']).size().unstack().reset_index()\n",
    "    alt_no_oh.columns = [i if i == 'EID' else 'alt_' + i for i in alt_no_oh.columns]\n",
    "\n",
    "    df['date'] = df['ALTDATE'].apply(translate_date)\n",
    "    date = df.groupby('EID')['date'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    date.columns = ['EID', 'alt_date_min', 'alt_date_max', 'alt_date_ptp', 'alt_date_std']\n",
    "\n",
    "    df['altbe'] = df['ALTBE'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    df['altaf'] = df['ALTAF'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    alt_be_af = df.groupby('EID')['altbe', 'altaf'].agg([min, max, np.mean]).reset_index()\n",
    "    alt_be_af.columns = ['EID', 'alt_be_min', 'alt_be_max', 'alt_be_mean', 'alt_af_min', 'alt_af_max', 'alt_af_mean']\n",
    "\n",
    "    mydf = pd.merge(alt_no, alt_no_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_right_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rig_type = df.groupby(['EID', 'RIGHTTYPE']).size().reset_index()\n",
    "    rig_type = rig_type.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rig_type.columns = ['EID', 'rig_count', 'rig_types_count']\n",
    "    \n",
    "    rig_type_oh_rate = df.groupby(['EID', 'RIGHTTYPE']).size().unstack().reset_index()\n",
    "    rig_type_oh_rate.iloc[:, 1:] = rig_type_oh_rate.iloc[:, 1:].div(rig_type['rig_count'], axis='index')\n",
    "    rig_type_oh_rate.columns = [i if i == 'EID' else 'rig_rate_' + str(i) for i in rig_type_oh_rate.columns]\n",
    "    \n",
    "    df['ask_month'] = (pd.to_datetime(df['ASKDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    ask_date = df.groupby('EID')['ask_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    ask_date.columns = ['EID', 'rig_askdate_max', 'rig_askdate_min', 'rig_askdate_ptp', 'rig_askdate_std']\n",
    "\n",
    "    df['get_month'] = (pd.to_datetime(df['FBDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    get_date = df.groupby('EID')['get_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    get_date.columns = ['EID', 'rig_getdate_max', 'rig_getdate_min', 'rig_getdate_ptp', 'rig_getdate_std']\n",
    "    \n",
    "    # bad\n",
    "    unget = df[df.FBDATE.isnull()]\n",
    "    unget = unget.groupby('EID').size().reset_index()\n",
    "    unget.columns = ['EID', 'rig_unget_num']\n",
    "    \n",
    "    right_1year = df[df['ASKDATE'] >= '2015-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_1year.columns = ['EID', 'ask_num(1year)']\n",
    "    right_2year = df[df['ASKDATE'] >= '2014-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_2year.columns = ['EID', 'ask_num(2year)']\n",
    "    right_5year = df[df['ASKDATE'] >= '2010-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_5year.columns = ['EID', 'ask_num(5year)']\n",
    "    right_end_1year = df[df['FBDATE'] >= '2015-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_1year.columns = ['EID', 'get_num(1year)']\n",
    "    right_end_2year = df[df['FBDATE'] >= '2014-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_2year.columns = ['EID', 'get_num(2year)']\n",
    "    right_end_5year = df[df['FBDATE'] >= '2010-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_5year.columns = ['EID', 'get_num(5year)']\n",
    "    \n",
    "    mydf = pd.merge(rig_type, rig_type_oh_rate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, get_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, unget, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_5year, how='left', on='EID')\n",
    "    \n",
    "    # bad\n",
    "    mydf['ask_rate(1year)'] = mydf['ask_num(1year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(2year)'] = mydf['ask_num(2year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(5year)'] = mydf['ask_num(5year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(1year)'] = mydf['get_num(1year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(2year)'] = mydf['get_num(2year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(5year)'] = mydf['get_num(5year)'] / mydf['rig_count']\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recruit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rec_wz = df.groupby(['EID', 'WZCODE']).size().reset_index()\n",
    "    rec_wz = rec_wz.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_wz.columns = ['EID', 'rec_wz_count', 'rec_wz_types_count']\n",
    "    \n",
    "    # bad\n",
    "    rec_wz_oh = df.groupby(['EID', 'WZCODE']).size().unstack().reset_index()\n",
    "    rec_wz_oh.columns = [i if i == 'EID' else 'rec_wz_' + i for i in rec_wz_oh.columns]\n",
    "    \n",
    "    # bad\n",
    "    rec_pos = df.groupby(['EID', 'POSCODE']).size().reset_index()\n",
    "    rec_pos = rec_pos.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_pos.columns = ['EID', 'rec_pos_count', 'rec_pos_types_count']\n",
    "    \n",
    "    df['recdate'] = (pd.to_datetime(df['RECDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    rec_date = df.groupby('EID')['recdate'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_date.columns = ['EID', 'rec_date_max', 'rec_date_min', 'rec_date_ptp', 'rec_date_std']\n",
    "    \n",
    "    # bad\n",
    "    df['pnum'] = df['PNUM'].str.extract('(\\d+)').fillna(1).astype(int)  # 若干=1\n",
    "    rec_num = df.groupby('EID')['pnum'].agg([sum, max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_num.columns = ['EID' if i == 'EID' else 'rec_num_' + i for i in rec_num.columns]\n",
    "    \n",
    "    mydf = pd.merge(rec_wz, rec_wz_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_pos, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branch_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bra_num = df.groupby('EID')['TYPECODE'].size().reset_index()\n",
    "    bra_num.columns = ['EID', 'bra_count']\n",
    "    \n",
    "    # bad\n",
    "    bra_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    bra_home.columns = ['EID', 'bra_nothome', 'bra_home']\n",
    "    \n",
    "    bra_year = df.groupby('EID')['B_REYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_year.columns = [i if i == 'EID' else 'bra_year_' + i for i in bra_year.columns]\n",
    "    \n",
    "    bra_endyear = df.groupby('EID')['B_ENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_endyear.columns = [i if i == 'EID' else 'bra_endyear_' + i for i in bra_endyear.columns]\n",
    "    \n",
    "    bra_end_num = df[~df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_end_num.columns = ['EID', 'bra_end_num']\n",
    "    bra_notend_num = df[df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_notend_num.columns = ['EID', 'bra_notend_num']\n",
    "    \n",
    "    mydf = pd.merge(bra_num, bra_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_end_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_notend_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_invest_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    inv_num = df.groupby('EID').size().reset_index()\n",
    "    inv_num.columns = ['EID', 'inv_count']\n",
    "    \n",
    "    # bad\n",
    "    inv_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inv_home.columns = ['EID', 'inv_nothome_num', 'inv_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inv_bl = df.groupby('EID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_bl.columns = [i if i == 'EID' else 'inv_bl_' + i for i in inv_bl.columns]\n",
    "    \n",
    "    inv_year = df.groupby('EID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_year.columns = [i if i == 'EID' else 'inv_year_' + i for i in inv_year.columns]\n",
    "    \n",
    "    # bad\n",
    "    inv_endyear = df.groupby('EID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_endyear.columns = [i if i == 'EID' else 'inv_endyear_' + i for i in inv_endyear.columns]\n",
    "    \n",
    "    # bad\n",
    "    inved_num = df.groupby('BTEID').size().reset_index()\n",
    "    inved_num.columns = ['EID', 'inved_num']\n",
    "    \n",
    "    inved_home = df.groupby(['BTEID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inved_home.columns = ['EID', 'inved_nothome_num', 'inved_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inved_bl = df.groupby('BTEID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_bl.columns = ['EID' if i == 'BTEID' else 'inved_bl_' + i for i in inved_bl.columns]\n",
    "    \n",
    "    inved_year = df.groupby('BTEID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_year.columns = ['EID' if i == 'BTEID' else 'inved_year_' + i for i in inved_year.columns]\n",
    "    \n",
    "    inved_endyear = df.groupby('BTEID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_endyear.columns = ['EID' if i == 'BTEID' else 'inved_endyear_' + i for i in inved_endyear.columns]\n",
    "    \n",
    "    mydf = pd.merge(inv_num, inv_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_endyear, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lawsuit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    law_num = df.groupby('EID').size().reset_index()\n",
    "    law_num.columns = ['EID', 'law_count']\n",
    "    \n",
    "    # bad\n",
    "    df['lawdate'] = df['LAWDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    law_date = df.groupby('EID')['lawdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    law_date.columns = [i if i == 'EID' else 'law_date_' + i for i in law_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    law_amout = df.groupby('EID')['LAWAMOUNT'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    law_amout.columns = [i if i == 'EID' else 'law_amout_' + i for i in law_amout.columns]\n",
    "    \n",
    "    mydf = pd.merge(law_num, law_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, law_amout, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_project_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    pro_num = df.groupby('EID').size().reset_index()\n",
    "    pro_num.columns = ['EID', 'pro_count']\n",
    "    \n",
    "    df['djdate'] = df['DJDATE'].apply(translate_date)\n",
    "    pro_date = df.groupby('EID')['djdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    pro_date.columns = [i if i == 'EID' else 'pro_date_' + i for i in pro_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    pro_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    pro_home.columns = ['EID', 'pro_nothome_num', 'pro_home_num']\n",
    "    \n",
    "    mydf = pd.merge(pro_num, pro_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, pro_home, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qualification_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    qua_num = df.groupby('EID').size().reset_index()\n",
    "    qua_num.columns = ['EID', 'qua_count']\n",
    "    \n",
    "    # bad\n",
    "    qua_type = df.groupby(['EID', 'ADDTYPE']).size().unstack().reset_index()\n",
    "    qua_type.columns = [i if i == 'EID' else 'qua_type_' + str(i) for i in qua_type.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['begindate'] = df['BEGINDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '')).apply(translate_date)\n",
    "    qua_begindate = df.groupby('EID')['begindate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_begindate.columns = [i if i == 'EID' else 'qua_begindate_' + i for i in qua_begindate.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['expirydate'] = df['EXPIRYDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '') if not pd.isnull(x) else np.nan)\n",
    "    df['expirydate'] = (pd.to_datetime(df['expirydate']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    qua_expirydate = df.groupby('EID')['expirydate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_expirydate.columns = [i if i == 'EID' else 'qua_expirydate_' + i for i in qua_expirydate.columns]\n",
    "    \n",
    "    mydf = pd.merge(qua_num, qua_type, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begindate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_expirydate, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_breakfaith_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bre_num = df.groupby('EID').size().reset_index()\n",
    "    bre_num.columns = ['EID', 'bre_count']\n",
    "    \n",
    "    # bad\n",
    "    df['fbdate'] = df['FBDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    bre_date = df.groupby('EID')['fbdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_date.columns = [i if i == 'EID' else 'bre_date_' + i for i in bre_date.columns]\n",
    "    \n",
    "    df['sxenddate'] = (pd.to_datetime(df['SXENDDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    bre_enddate = df.groupby('EID')['sxenddate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_enddate.columns = [i if i == 'EID' else 'bre_enddate_' + i for i in bre_enddate.columns]\n",
    "    \n",
    "    mydf = pd.merge(bre_num, bre_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_enddate, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "entbase_feat = get_entbase_feature(entbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:16: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "alter_feat = get_alter_feature(alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_feature = get_right_feature(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:22: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "recruit_feat = get_recruit_feature(recruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_feat = get_branch_feature(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_feat = get_invest_feature(invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lawsuit_feat = get_lawsuit_feature(lawsuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_feat = get_project_feature(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualification_feat = get_qualification_feature(qualification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfaith_feat = get_breakfaith_feature(breakfaith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, right_feature, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, recruit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, branch_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, invest_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, lawsuit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, project_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, qualification_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, breakfaith_feat, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = pd.merge(train, dataset, on='EID', how='left')\n",
    "testset = pd.merge(test, dataset, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_index = testset.EID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# EID 前面的字母代表不同省份，已提供了 PROV 列，因此字母是冗余信息，直接舍弃\n",
    "trainset['EID'] = trainset['EID'].str.extract('(\\d+)').astype(int)\n",
    "testset['EID'] = testset['EID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218264, 278) (218264L,) (218247, 278)\n"
     ]
    }
   ],
   "source": [
    "train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)\n",
    "train_label = trainset.TARGET.values\n",
    "test_feature = testset\n",
    "# test_index = testset.EID.values\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 3\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "#     'objective': 'rank:pairwise',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights ': 0,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 20,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, params, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = xgb.cv(params, dtrain, num_round, verbose_eval=10, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res), res.loc[len(res) - 1, 'test-auc-mean']\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=50)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, threshold, name):\n",
    "    result = pd.DataFrame({'EID': test_index, 'FORTARGET': 0, 'PROB': pred})\n",
    "    mask = result['PROB'] >= threshold\n",
    "    result.at[mask, 'FORTARGET'] = 1\n",
    "    # result['PROB'] = result['PROB'].apply(lambda x: round(x, 4))\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'EID', u'PROV', u'RGYEAR', u'HY', u'ZCZB', u'ETYPE', u'MPNUM', u'INUM',\n",
      "       u'ENUM', u'FINZB',\n",
      "       ...\n",
      "       u'qua_expirydate_std', u'bre_count', u'bre_date_min', u'bre_date_max',\n",
      "       u'bre_date_ptp', u'bre_date_std', u'bre_enddate_min',\n",
      "       u'bre_enddate_max', u'bre_enddate_ptp', u'bre_enddate_std'],\n",
      "      dtype='object', length=278)\n",
      "run cv: round: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "[0]\tcv-test-auc:0.644712333333+0.000670085235043\tcv-train-auc:0.662312333333+0.00566043286527\n",
      "[10]\tcv-test-auc:0.663217333333+0.00213649406479\tcv-train-auc:0.703540666667+0.0025598387102\n",
      "[20]\tcv-test-auc:0.666214+0.00211630448345\tcv-train-auc:0.709414666667+0.0016057698742\n",
      "[30]\tcv-test-auc:0.668307666667+0.00268354707223\tcv-train-auc:0.713202+0.00130562654181\n",
      "[40]\tcv-test-auc:0.669617333333+0.00238255106043\tcv-train-auc:0.716113333333+0.00114880121092\n",
      "[50]\tcv-test-auc:0.670759333333+0.00229190885993\tcv-train-auc:0.719176666667+0.00138755764645\n",
      "[60]\tcv-test-auc:0.671459666667+0.00224125579878\tcv-train-auc:0.721516+0.00110392058893\n",
      "[70]\tcv-test-auc:0.672181666667+0.00231294074479\tcv-train-auc:0.724293666667+0.001424848374\n",
      "[80]\tcv-test-auc:0.672769666667+0.00229396677298\tcv-train-auc:0.726728666667+0.00122820365666\n",
      "[90]\tcv-test-auc:0.673219+0.00234038073826\tcv-train-auc:0.729033666667+0.00105382678315\n",
      "[100]\tcv-test-auc:0.673604333333+0.00220681268399\tcv-train-auc:0.731462666667+0.00104256105603\n",
      "[110]\tcv-test-auc:0.674043333333+0.00210532124759\tcv-train-auc:0.73377+0.000965095159384\n",
      "[120]\tcv-test-auc:0.674637+0.00207432736086\tcv-train-auc:0.736026333333+0.00113207017843\n",
      "[130]\tcv-test-auc:0.675270666667+0.00207673531187\tcv-train-auc:0.738052666667+0.00121578680514\n",
      "[140]\tcv-test-auc:0.675821+0.00206060816265\tcv-train-auc:0.740217+0.00123422229224\n",
      "[150]\tcv-test-auc:0.676273666667+0.00205555934534\tcv-train-auc:0.742161333333+0.00124121052024\n",
      "[160]\tcv-test-auc:0.676717+0.00207804972671\tcv-train-auc:0.743926+0.00109665521777\n",
      "[170]\tcv-test-auc:0.677281666667+0.00214710973688\tcv-train-auc:0.745762+0.00105359195137\n",
      "[180]\tcv-test-auc:0.677748666667+0.00207529698009\tcv-train-auc:0.747733+0.00126657648802\n",
      "[190]\tcv-test-auc:0.678312+0.0021318922737\tcv-train-auc:0.749625666667+0.00122835454527\n",
      "[200]\tcv-test-auc:0.678705666667+0.00208465382151\tcv-train-auc:0.751504666667+0.00121509514945\n",
      "[210]\tcv-test-auc:0.679052333333+0.00204972068558\tcv-train-auc:0.753266666667+0.0012033980592\n",
      "[220]\tcv-test-auc:0.679506+0.00200581969944\tcv-train-auc:0.75511+0.00123021976357\n",
      "[230]\tcv-test-auc:0.679862333333+0.00198069791291\tcv-train-auc:0.756653666667+0.00125903993406\n",
      "[240]\tcv-test-auc:0.680226333333+0.00188518740595\tcv-train-auc:0.758142+0.00133108552192\n",
      "[250]\tcv-test-auc:0.680528+0.00185595204679\tcv-train-auc:0.759733333333+0.00148649034829\n",
      "[260]\tcv-test-auc:0.680791+0.00184193611905\tcv-train-auc:0.761378333333+0.00149460570795\n",
      "[270]\tcv-test-auc:0.681141333333+0.00181982001552\tcv-train-auc:0.762841333333+0.00144298494641\n",
      "[280]\tcv-test-auc:0.681328333333+0.0018556474761\tcv-train-auc:0.764226333333+0.00134715263509\n",
      "[290]\tcv-test-auc:0.68158+0.00185328303289\tcv-train-auc:0.765565333333+0.00133695483178\n",
      "[300]\tcv-test-auc:0.681881666667+0.00181644383955\tcv-train-auc:0.766935666667+0.0013606376773\n",
      "[310]\tcv-test-auc:0.682104333333+0.00183028674135\tcv-train-auc:0.768158+0.00130641366598\n",
      "[320]\tcv-test-auc:0.682328+0.00182884736013\tcv-train-auc:0.769281666667+0.00131296974663\n",
      "[330]\tcv-test-auc:0.682521+0.00187432352241\tcv-train-auc:0.770383+0.00127738587227\n",
      "[340]\tcv-test-auc:0.682685666667+0.00188790401121\tcv-train-auc:0.771643666667+0.001327392515\n",
      "[350]\tcv-test-auc:0.682918+0.0019685065405\tcv-train-auc:0.772800333333+0.00122267611774\n",
      "[360]\tcv-test-auc:0.683062+0.00196801947822\tcv-train-auc:0.774012666667+0.00118096636512\n",
      "[370]\tcv-test-auc:0.683217+0.00197388770366\tcv-train-auc:0.775065+0.00113478896717\n",
      "[380]\tcv-test-auc:0.683355333333+0.00195548499241\tcv-train-auc:0.776075333333+0.00117667110481\n",
      "[390]\tcv-test-auc:0.683503666667+0.00193879934209\tcv-train-auc:0.777150333333+0.00105107130533\n",
      "[400]\tcv-test-auc:0.683627333333+0.00197186702279\tcv-train-auc:0.778185666667+0.00101056958637\n",
      "[410]\tcv-test-auc:0.683771666667+0.00195432670986\tcv-train-auc:0.779297+0.000881716885778\n",
      "[420]\tcv-test-auc:0.683859666667+0.00192685310517\tcv-train-auc:0.780282333333+0.000942126082621\n",
      "[430]\tcv-test-auc:0.683987+0.00196173256757\tcv-train-auc:0.781326666667+0.000939245205234\n",
      "[440]\tcv-test-auc:0.684162333333+0.00199957367678\tcv-train-auc:0.782269333333+0.000791529040669\n",
      "[450]\tcv-test-auc:0.684259333333+0.00195129945307\tcv-train-auc:0.783282333333+0.000759108834241\n",
      "[460]\tcv-test-auc:0.684332333333+0.00194616654535\tcv-train-auc:0.784173333333+0.000747576231713\n",
      "[470]\tcv-test-auc:0.684397666667+0.00196411308794\tcv-train-auc:0.785035333333+0.000775680489778\n",
      "[480]\tcv-test-auc:0.684489+0.00198149236688\tcv-train-auc:0.785984+0.000749615012301\n",
      "[490]\tcv-test-auc:0.684577333333+0.00200535056509\tcv-train-auc:0.786789+0.000760384113458\n",
      "[500]\tcv-test-auc:0.684728+0.00198784506439\tcv-train-auc:0.787693333333+0.000701758426304\n",
      "[510]\tcv-test-auc:0.684799333333+0.00198895958285\tcv-train-auc:0.788637+0.000500370529375\n",
      "[520]\tcv-test-auc:0.684914+0.0019669073864\tcv-train-auc:0.789667+0.000344488993535\n",
      "[530]\tcv-test-auc:0.684963333333+0.00194999646723\tcv-train-auc:0.790557333333+0.000277341346038\n",
      "[540]\tcv-test-auc:0.68503+0.00193310855015\tcv-train-auc:0.791348666667+0.000244298087499\n",
      "[550]\tcv-test-auc:0.685122+0.00196203822593\tcv-train-auc:0.792324666667+0.000167782266312\n",
      "[560]\tcv-test-auc:0.685201666667+0.00195186651411\tcv-train-auc:0.793224+8.36540495135e-05\n",
      "[570]\tcv-test-auc:0.685250333333+0.00196903789253\tcv-train-auc:0.794103666667+9.87803399243e-05\n",
      "[580]\tcv-test-auc:0.685356+0.00194815673565\tcv-train-auc:0.795102333333+0.000195124461705\n",
      "[590]\tcv-test-auc:0.685439666667+0.00195536157498\tcv-train-auc:0.796043+0.00030281017156\n",
      "[600]\tcv-test-auc:0.685487+0.00194490119715\tcv-train-auc:0.796958+0.00018213731084\n",
      "[610]\tcv-test-auc:0.685495+0.00193012970203\tcv-train-auc:0.797928666667+0.000324545151798\n",
      "[620]\tcv-test-auc:0.685560333333+0.00193248929852\tcv-train-auc:0.798784666667+0.000266173293593\n",
      "[630]\tcv-test-auc:0.685591666667+0.00197513819488\tcv-train-auc:0.799518+0.000143571120587\n",
      "[640]\tcv-test-auc:0.685646333333+0.00196172463806\tcv-train-auc:0.800455333333+0.000241757913808\n",
      "[650]\tcv-test-auc:0.685672333333+0.00199995238832\tcv-train-auc:0.80121+0.000344268306219\n",
      "[660]\tcv-test-auc:0.685744+0.00198163888402\tcv-train-auc:0.802066333333+0.000366028535621\n",
      "[670]\tcv-test-auc:0.685782333333+0.00201277094794\tcv-train-auc:0.802781333333+0.000369007979438\n",
      "[680]\tcv-test-auc:0.685837+0.00201473323296\tcv-train-auc:0.803589+0.000477596063635\n",
      "[690]\tcv-test-auc:0.685884666667+0.00199511007772\tcv-train-auc:0.804467+0.000737699577516\n",
      "[700]\tcv-test-auc:0.685925333333+0.00201544045365\tcv-train-auc:0.805568+0.000756376008786\n",
      "[710]\tcv-test-auc:0.685959+0.00199737143933\tcv-train-auc:0.806308+0.000732164371345\n",
      "[720]\tcv-test-auc:0.685991+0.00202826083135\tcv-train-auc:0.807278+0.000840734599423\n",
      "[730]\tcv-test-auc:0.686040333333+0.00203433401606\tcv-train-auc:0.808062666667+0.000820228558933\n",
      "[740]\tcv-test-auc:0.686056666667+0.00203650752242\tcv-train-auc:0.808802666667+0.000809193974822\n",
      "[750]\tcv-test-auc:0.686045+0.00204634812939\tcv-train-auc:0.809532666667+0.000784591755133\n",
      "[760]\tcv-test-auc:0.686092666667+0.00204647930739\tcv-train-auc:0.810419666667+0.000812653814501\n",
      "[770]\tcv-test-auc:0.686124333333+0.00206536039363\tcv-train-auc:0.811379333333+0.000890197481212\n",
      "[780]\tcv-test-auc:0.686123333333+0.00207471802636\tcv-train-auc:0.812092+0.000872201811509\n",
      "[790]\tcv-test-auc:0.686132+0.00205646346916\tcv-train-auc:0.812937333333+0.000693361057138\n",
      "[800]\tcv-test-auc:0.686117333333+0.00203356998623\tcv-train-auc:0.813655333333+0.000724971876849\n",
      "[810]\tcv-test-auc:0.686146+0.00204303303938\tcv-train-auc:0.814453+0.000881942174975\n",
      "[820]\tcv-test-auc:0.686167333333+0.00206241999171\tcv-train-auc:0.815245+0.000847935532141\n",
      "[830]\tcv-test-auc:0.68618+0.0020376934673\tcv-train-auc:0.815941+0.000914582236142\n",
      "[840]\tcv-test-auc:0.686201333333+0.00204578270813\tcv-train-auc:0.816665333333+0.000875834966697\n",
      "[850]\tcv-test-auc:0.686229666667+0.002057167038\tcv-train-auc:0.817376333333+0.00093369314493\n",
      "[860]\tcv-test-auc:0.686258666667+0.00207595894843\tcv-train-auc:0.818076666667+0.00102054114186\n",
      "[870]\tcv-test-auc:0.686257666667+0.00209333805732\tcv-train-auc:0.818929666667+0.000964835852476\n",
      "[880]\tcv-test-auc:0.686264+0.00208512557575\tcv-train-auc:0.819708666667+0.00109437511343\n",
      "[890]\tcv-test-auc:0.686272+0.00210394597523\tcv-train-auc:0.820377666667+0.00116040720822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[900]\tcv-test-auc:0.686298666667+0.00215144623193\tcv-train-auc:0.821192+0.0012033722062\n",
      "[910]\tcv-test-auc:0.686296666667+0.00215409831613\tcv-train-auc:0.822041333333+0.0011225305737\n",
      "[920]\tcv-test-auc:0.686315666667+0.00215626858768\tcv-train-auc:0.822749333333+0.00103778813937\n",
      "[930]\tcv-test-auc:0.686323666667+0.00213326359261\tcv-train-auc:0.823482+0.00104936202841\n",
      "[940]\tcv-test-auc:0.686313666667+0.00215461246838\tcv-train-auc:0.824087333333+0.00107854541964\n",
      "[950]\tcv-test-auc:0.686315+0.00216174019407\tcv-train-auc:0.824828+0.00102811575224\n",
      "[960]\tcv-test-auc:0.686298+0.00215482543763\tcv-train-auc:0.825519666667+0.000984552803166\n",
      "[970]\tcv-test-auc:0.686303+0.00215723032305\tcv-train-auc:0.826320666667+0.00094372182813\n",
      "[980]\tcv-test-auc:0.686325333333+0.00215150340202\tcv-train-auc:0.826977333333+0.000915078260163\n",
      "[990]\tcv-test-auc:0.686315333333+0.00214954387927\tcv-train-auc:0.827695666667+0.000847523582104\n",
      "[1000]\tcv-test-auc:0.686353+0.00213850976149\tcv-train-auc:0.828477666667+0.000824990639678\n",
      "[1010]\tcv-test-auc:0.686364333333+0.00213616811656\tcv-train-auc:0.829317666667+0.000840974566929\n",
      "[1020]\tcv-test-auc:0.686376666667+0.00211477994652\tcv-train-auc:0.829973+0.000851812577194\n",
      "[1030]\tcv-test-auc:0.686394333333+0.0021265700605\tcv-train-auc:0.830666666667+0.000871750856355\n",
      "[1040]\tcv-test-auc:0.686382333333+0.00212171193982\tcv-train-auc:0.831399666667+0.000772409361817\n",
      "[1050]\tcv-test-auc:0.686390333333+0.00212371095543\tcv-train-auc:0.832129333333+0.000786958844046\n",
      "[1060]\tcv-test-auc:0.686395333333+0.00211158650203\tcv-train-auc:0.832778+0.000746569487724\n",
      "[1070]\tcv-test-auc:0.686419666667+0.00212125816963\tcv-train-auc:0.833482333333+0.000707540497467\n",
      "[1080]\tcv-test-auc:0.686402333333+0.00211921751807\tcv-train-auc:0.834159+0.000689487249096\n",
      "[1090]\tcv-test-auc:0.686438+0.00208497689835\tcv-train-auc:0.834847666667+0.000744449834143\n",
      "[1100]\tcv-test-auc:0.686419+0.0020908172246\tcv-train-auc:0.835515666667+0.000726890332551\n",
      "[1110]\tcv-test-auc:0.686443666667+0.00209700108621\tcv-train-auc:0.836264333333+0.000736920318322\n",
      "[1120]\tcv-test-auc:0.686440333333+0.00206881066208\tcv-train-auc:0.836991333333+0.000681914136009\n",
      "[1130]\tcv-test-auc:0.686435+0.00206346181614\tcv-train-auc:0.837681666667+0.000592454967815\n",
      "[1140]\tcv-test-auc:0.686428666667+0.00207547301168\tcv-train-auc:0.838447666667+0.000517796184699\n",
      "[1150]\tcv-test-auc:0.686445+0.00207578226218\tcv-train-auc:0.839035333333+0.000373395530533\n",
      "[1160]\tcv-test-auc:0.686470666667+0.00206523063011\tcv-train-auc:0.839793+0.000397284113283\n",
      "[1170]\tcv-test-auc:0.686476666667+0.00210253978691\tcv-train-auc:0.840452333333+0.000432862051415\n",
      "[1180]\tcv-test-auc:0.686467333333+0.00205064418063\tcv-train-auc:0.841039333333+0.000465378937593\n",
      "[1190]\tcv-test-auc:0.686497+0.00206702507645\tcv-train-auc:0.841668333333+0.000427591965417\n",
      "[1200]\tcv-test-auc:0.686483+0.00206043215532\tcv-train-auc:0.842370666667+0.000390999005398\n",
      "[1210]\tcv-test-auc:0.686449333333+0.00205373908329\tcv-train-auc:0.842994333333+0.000326513229271\n",
      "[1220]\tcv-test-auc:0.686446333333+0.00206014665066\tcv-train-auc:0.843609333333+0.000361866764923\n",
      "[1230]\tcv-test-auc:0.686427333333+0.00205239832608\tcv-train-auc:0.844304333333+0.000317087089334\n",
      "[1240]\tcv-test-auc:0.686413666667+0.00204981402951\tcv-train-auc:0.844915333333+0.00029227080289\n",
      "[1250]\tcv-test-auc:0.686425333333+0.00201864151239\tcv-train-auc:0.845472666667+0.000407076023476\n",
      "[1260]\tcv-test-auc:0.686408+0.00202332202084\tcv-train-auc:0.846035333333+0.000382946761603\n",
      "[1270]\tcv-test-auc:0.686406666667+0.00203306096536\tcv-train-auc:0.846675666667+0.000329490853018\n",
      "[1280]\tcv-test-auc:0.686424333333+0.00203218442951\tcv-train-auc:0.847278666667+0.000335980488852\n",
      "[1290]\tcv-test-auc:0.686383666667+0.00200387164149\tcv-train-auc:0.847899666667+0.00033995522581\n",
      "Stopping. Best iteration:\n",
      "[1194] cv-mean:0.686497333333\tcv-std:0.00205110577223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 1420.2224206 s\n"
     ]
    }
   ],
   "source": [
    "iterations, best_score = xgb_cv(train_feature, train_label, params, config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.659421\n",
      "[50]\ttrain-auc:0.710104\n",
      "[100]\ttrain-auc:0.719463\n",
      "[150]\ttrain-auc:0.728780\n",
      "[200]\ttrain-auc:0.736664\n",
      "[250]\ttrain-auc:0.744007\n",
      "[300]\ttrain-auc:0.749715\n",
      "[350]\ttrain-auc:0.754900\n",
      "[400]\ttrain-auc:0.759926\n",
      "[450]\ttrain-auc:0.764106\n",
      "[500]\ttrain-auc:0.768829\n",
      "[550]\ttrain-auc:0.772715\n",
      "[600]\ttrain-auc:0.776605\n",
      "[650]\ttrain-auc:0.780680\n",
      "[700]\ttrain-auc:0.784270\n",
      "[750]\ttrain-auc:0.787792\n",
      "[800]\ttrain-auc:0.791761\n",
      "[850]\ttrain-auc:0.795238\n",
      "[900]\ttrain-auc:0.798605\n",
      "[950]\ttrain-auc:0.801859\n",
      "[1000]\ttrain-auc:0.805212\n",
      "[1050]\ttrain-auc:0.808278\n",
      "[1100]\ttrain-auc:0.811090\n",
      "[1150]\ttrain-auc:0.814038\n",
      "[1194]\ttrain-auc:0.816332\n"
     ]
    }
   ],
   "source": [
    "model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(model.get_fscore().items(), columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "importance.to_csv('../data/output/feat_imp/importance-1127-%f.csv' % best_score, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, 0.21, '1127-xgb-%f' % best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
