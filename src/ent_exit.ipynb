{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/input/train.csv')\n",
    "entbase = pd.read_csv('../data/input/1entbase.csv')\n",
    "alter = pd.read_csv('../data/input/2alter.csv')\n",
    "branch = pd.read_csv('../data/input/3branch.csv')\n",
    "invest = pd.read_csv('../data/input/4invest.csv')\n",
    "right = pd.read_csv('../data/input/5right.csv')\n",
    "project = pd.read_csv('../data/input/6project.csv')\n",
    "lawsuit = pd.read_csv('../data/input/7lawsuit.csv')\n",
    "breakfaith = pd.read_csv('../data/input/8breakfaith.csv')\n",
    "recruit = pd.read_csv('../data/input/9recruit.csv')\n",
    "qualification = pd.read_csv('../data/input/10qualification.csv', encoding='gbk')\n",
    "test = pd.read_csv('../data/input/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[-2:])\n",
    "    return (year - 2010) * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_interaction_feature(df, feature_A, feature_B):\n",
    "    feature_A_list = sorted(df[feature_A].unique())\n",
    "    feature_B_list = sorted(df[feature_B].unique())\n",
    "    count = 0\n",
    "    mydict = {}\n",
    "    for i in feature_A_list:\n",
    "        mydict[int(i)] = {}\n",
    "        for j in feature_B_list:\n",
    "            mydict[int(i)][int(j)] = count\n",
    "            count += 1\n",
    "    return df.apply(lambda x: mydict[int(x[feature_A])][int(x[feature_B])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entbase_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mydf = df.fillna(value={'HY': 0, 'ZCZB': 0, 'MPNUM': 0, 'INUM': 0, 'ENUM': 0, 'FINZB': 0, 'FSTINUM': 0, 'TZINUM': 0})  # fill HY 为 0；ZCZB 为 0 表示缺失或错误\n",
    "\n",
    "    mydf['allnum'] = mydf['MPNUM'] + mydf['INUM'] + mydf['MPNUM'] + mydf['TZINUM'] + mydf['ENUM']\n",
    "\n",
    "    mydf['zczb/rgyear'] = mydf['ZCZB'] / mydf['RGYEAR']\n",
    "    mydf[\"rgyear_zczb\"] = get_interaction_feature(mydf, \"RGYEAR\", \"ZCZB\")\n",
    "    mydf['rgyear_finzb'] = get_interaction_feature(mydf, 'RGYEAR', 'FINZB')\n",
    "    mydf['rgyear_inum'] = get_interaction_feature(mydf, 'RGYEAR', 'INUM')\n",
    "    mydf['rgyear_enum'] = get_interaction_feature(mydf, 'RGYEAR', 'ENUM')\n",
    "    mydf['rgyear_fstinum'] = get_interaction_feature(mydf, 'RGYEAR', 'FSTINUM')\n",
    "    mydf['rgyear_tzinum'] = get_interaction_feature(mydf, 'RGYEAR', 'TZINUM')\n",
    "    mydf['rgyear_mpnum'] = get_interaction_feature(mydf, 'RGYEAR', 'MPNUM')\n",
    "    mydf['zczb_rgyear'] = get_interaction_feature(mydf, 'ZCZB', 'RGYEAR')\n",
    "    mydf['zczb_finzb'] = get_interaction_feature(mydf, 'ZCZB', 'FINZB')\n",
    "    mydf['zczb_inum'] = get_interaction_feature(mydf, 'ZCZB', 'INUM')\n",
    "    mydf['zczb_enum'] = get_interaction_feature(mydf, 'ZCZB', 'ENUM')\n",
    "    mydf['zczb_fstinum'] = get_interaction_feature(mydf, 'ZCZB', 'FSTINUM')\n",
    "    mydf['zczb_tzinum'] = get_interaction_feature(mydf, 'ZCZB', 'TZINUM')\n",
    "    mydf['zczb_mpnum'] = get_interaction_feature(mydf, 'ZCZB', 'MPNUM')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_alter_feature(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    alt_no = df.groupby(['EID', 'ALTERNO']).size().reset_index()\n",
    "    alt_no = alt_no.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    alt_no.columns = ['EID', 'alt_count', 'alt_types_count']\n",
    "\n",
    "    alt_no_oh = df.groupby(['EID', 'ALTERNO']).size().unstack().reset_index()\n",
    "    alt_no_oh.columns = [i if i == 'EID' else 'alt_' + i for i in alt_no_oh.columns]\n",
    "\n",
    "    df['date'] = df['ALTDATE'].apply(translate_date)\n",
    "    date = df.groupby('EID')['date'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    date.columns = ['EID', 'alt_date_min', 'alt_date_max', 'alt_date_ptp', 'alt_date_std']\n",
    "\n",
    "    df['altbe'] = df['ALTBE'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    df['altaf'] = df['ALTAF'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    alt_be_af = df.groupby('EID')['altbe', 'altaf'].agg([min, max, np.mean]).reset_index()\n",
    "    alt_be_af.columns = ['EID', 'alt_be_min', 'alt_be_max', 'alt_be_mean', 'alt_af_min', 'alt_af_max', 'alt_af_mean']\n",
    "    \n",
    "#     tmp = df.groupby(df.columns.tolist(), as_index=False).size().reset_index()\n",
    "#     tmp = tmp.drop(['ALTERNO', 'ALTDATE', 'ALTBE', 'ALTAF', 'date', 'altbe', 'altaf'], axis=1)\n",
    "#     tmp.columns = ['EID', 'alt_dup_count']\n",
    "#     alt_dup_count = tmp.groupby(['EID', 'alt_dup_count']).size().unstack().reset_index()\n",
    "#     alt_dup_count.columns = [i if i == 'EID' else 'alt_dup_count_' + str(i) for i in alt_dup_count]\n",
    "    \n",
    "#     alt_dup = tmp[tmp['alt_dup_count'] > 1].groupby('EID')['alt_dup_count'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "#     alt_dup.columns = [i if i == 'EID' else 'alt_dup_' + i for i in alt_dup]\n",
    "\n",
    "    df['alt_be_af_gap'] = df['altaf'] - df['altbe']\n",
    "    alt_be_af_gap = df.groupby('EID')['alt_be_af_gap'].agg([min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    alt_be_af_gap.columns = [i if i == 'EID' else 'alt_be_af_gap_' + i for i in alt_be_af_gap.columns]\n",
    "    \n",
    "    alt_1year = df[df['ALTDATE'] >= '2015-01'].groupby('EID').size().reset_index()\n",
    "    alt_1year.columns = ['EID', 'alt_num(1year)']\n",
    "    alt_2year = df[df['ALTDATE'] >= '2014-01'].groupby('EID').size().reset_index()\n",
    "    alt_2year.columns = ['EID', 'alt_num(2year)']\n",
    "    alt_3year = df[df['ALTDATE'] >= '2013-01'].groupby('EID').size().reset_index()\n",
    "    alt_3year.columns = ['EID', 'alt_num(3year)']\n",
    "\n",
    "    mydf = pd.merge(alt_no, alt_no_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af_gap, how='left', on='EID')\n",
    "#     mydf = pd.merge(mydf, alt_dup_count, how='left', on='EID')\n",
    "#     mydf = pd.merge(mydf, alt_dup, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_3year, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_right_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rig_type = df.groupby(['EID', 'RIGHTTYPE']).size().reset_index()\n",
    "    rig_type = rig_type.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rig_type.columns = ['EID', 'rig_count', 'rig_types_count']\n",
    "    \n",
    "    rig_type_oh_rate = df.groupby(['EID', 'RIGHTTYPE']).size().unstack().reset_index()\n",
    "    rig_type_oh_rate.iloc[:, 1:] = rig_type_oh_rate.iloc[:, 1:].div(rig_type['rig_count'], axis='index')\n",
    "    rig_type_oh_rate.columns = [i if i == 'EID' else 'rig_rate_' + str(i) for i in rig_type_oh_rate.columns]\n",
    "    \n",
    "    df['ask_month'] = (pd.to_datetime(df['ASKDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    ask_date = df.groupby('EID')['ask_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    ask_date.columns = ['EID', 'rig_askdate_max', 'rig_askdate_min', 'rig_askdate_ptp', 'rig_askdate_std']\n",
    "\n",
    "    df['get_month'] = (pd.to_datetime(df['FBDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    get_date = df.groupby('EID')['get_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    get_date.columns = ['EID', 'rig_getdate_max', 'rig_getdate_min', 'rig_getdate_ptp', 'rig_getdate_std']\n",
    "    \n",
    "    # bad\n",
    "    unget = df[df.FBDATE.isnull()]\n",
    "    unget = unget.groupby('EID').size().reset_index()\n",
    "    unget.columns = ['EID', 'rig_unget_num']\n",
    "    \n",
    "    right_1year = df[df['ASKDATE'] >= '2015-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_1year.columns = ['EID', 'ask_num(1year)']\n",
    "    right_2year = df[df['ASKDATE'] >= '2014-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_2year.columns = ['EID', 'ask_num(2year)']\n",
    "    right_5year = df[df['ASKDATE'] >= '2010-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_5year.columns = ['EID', 'ask_num(5year)']\n",
    "    right_end_1year = df[df['FBDATE'] >= '2015-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_1year.columns = ['EID', 'get_num(1year)']\n",
    "    right_end_2year = df[df['FBDATE'] >= '2014-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_2year.columns = ['EID', 'get_num(2year)']\n",
    "    right_end_5year = df[df['FBDATE'] >= '2010-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_5year.columns = ['EID', 'get_num(5year)']\n",
    "    \n",
    "    df['ask_get_month_gap'] = df['get_month'] - df['ask_month']\n",
    "    ask_get_month_gap = df.groupby('EID')['ask_get_month_gap'].agg([max, min, np.ptp, np.mean, np.std]).reset_index()\n",
    "    ask_get_month_gap.columns = [i if i == 'EID' else 'ask_get_month_gap_' + i for i in ask_get_month_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(rig_type, rig_type_oh_rate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, get_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, unget, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_get_month_gap, how='left', on='EID')\n",
    "    \n",
    "    # bad\n",
    "    mydf['ask_rate(1year)'] = mydf['ask_num(1year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(2year)'] = mydf['ask_num(2year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(5year)'] = mydf['ask_num(5year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(1year)'] = mydf['get_num(1year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(2year)'] = mydf['get_num(2year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(5year)'] = mydf['get_num(5year)'] / mydf['rig_count']\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recruit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rec_wz = df.groupby(['EID', 'WZCODE']).size().reset_index()\n",
    "    rec_wz = rec_wz.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_wz.columns = ['EID', 'rec_wz_count', 'rec_wz_types_count']\n",
    "    \n",
    "    # bad\n",
    "    rec_wz_oh = df.groupby(['EID', 'WZCODE']).size().unstack().reset_index()\n",
    "    rec_wz_oh.columns = [i if i == 'EID' else 'rec_wz_' + i for i in rec_wz_oh.columns]\n",
    "    \n",
    "    # bad\n",
    "    rec_pos = df.groupby(['EID', 'POSCODE']).size().reset_index()\n",
    "    rec_pos = rec_pos.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_pos.columns = ['EID', 'rec_pos_count', 'rec_pos_types_count']\n",
    "    \n",
    "    df['recdate'] = (pd.to_datetime(df['RECDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    rec_date = df.groupby('EID')['recdate'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_date.columns = ['EID', 'rec_date_max', 'rec_date_min', 'rec_date_ptp', 'rec_date_std']\n",
    "    \n",
    "    # bad\n",
    "    df['pnum'] = df['PNUM'].str.extract('(\\d+)').fillna(1).astype(int)  # 若干=1\n",
    "    rec_num = df.groupby('EID')['pnum'].agg([sum, max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_num.columns = ['EID' if i == 'EID' else 'rec_num_' + i for i in rec_num.columns]\n",
    "    \n",
    "    rec_count = df.groupby('EID').size().reset_index()\n",
    "    rec_count.columns = ['EID', 'rec_count']\n",
    "    \n",
    "    mydf = pd.merge(rec_wz, rec_wz_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_pos, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_count, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branch_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bra_num = df.groupby('EID')['TYPECODE'].size().reset_index()\n",
    "    bra_num.columns = ['EID', 'bra_count']\n",
    "    \n",
    "    # bad\n",
    "    bra_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    bra_home.columns = ['EID', 'bra_nothome', 'bra_home']\n",
    "    \n",
    "    bra_year = df.groupby('EID')['B_REYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_year.columns = [i if i == 'EID' else 'bra_year_' + i for i in bra_year.columns]\n",
    "    \n",
    "    bra_endyear = df.groupby('EID')['B_ENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_endyear.columns = [i if i == 'EID' else 'bra_endyear_' + i for i in bra_endyear.columns]\n",
    "    \n",
    "    bra_end_num = df[~df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_end_num.columns = ['EID', 'bra_end_num']\n",
    "    bra_notend_num = df[df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_notend_num.columns = ['EID', 'bra_notend_num']\n",
    "    \n",
    "    df['bra_begin_end_gap'] = df['B_ENDYEAR'] - df['B_REYEAR']\n",
    "    bra_begin_end_gap = df.groupby('EID')['bra_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    bra_begin_end_gap.columns = [i if i == 'EID' else 'bra_begin_end_gap_' + i for i in bra_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(bra_num, bra_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_end_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_notend_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_begin_end_gap, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_invest_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    inv_num = df.groupby('EID').size().reset_index()\n",
    "    inv_num.columns = ['EID', 'inv_count']\n",
    "    \n",
    "    # bad\n",
    "    inv_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inv_home.columns = ['EID', 'inv_nothome_num', 'inv_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inv_bl = df.groupby('EID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_bl.columns = [i if i == 'EID' else 'inv_bl_' + i for i in inv_bl.columns]\n",
    "    \n",
    "    inv_year = df.groupby('EID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_year.columns = [i if i == 'EID' else 'inv_year_' + i for i in inv_year.columns]\n",
    "    \n",
    "    # bad\n",
    "    inv_endyear = df.groupby('EID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_endyear.columns = [i if i == 'EID' else 'inv_endyear_' + i for i in inv_endyear.columns]\n",
    "    \n",
    "    # bad\n",
    "    inved_num = df.groupby('BTEID').size().reset_index()\n",
    "    inved_num.columns = ['EID', 'inved_num']\n",
    "    \n",
    "    inved_home = df.groupby(['BTEID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inved_home.columns = ['EID', 'inved_nothome_num', 'inved_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inved_bl = df.groupby('BTEID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_bl.columns = ['EID' if i == 'BTEID' else 'inved_bl_' + i for i in inved_bl.columns]\n",
    "    \n",
    "    inved_year = df.groupby('BTEID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_year.columns = ['EID' if i == 'BTEID' else 'inved_year_' + i for i in inved_year.columns]\n",
    "    \n",
    "    inved_endyear = df.groupby('BTEID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_endyear.columns = ['EID' if i == 'BTEID' else 'inved_endyear_' + i for i in inved_endyear.columns]\n",
    "    \n",
    "    df['inv_begin_end_gap'] = df['BTENDYEAR'] - df['BTYEAR']\n",
    "    inv_begin_end_gap = df.groupby('EID')['inv_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    inv_begin_end_gap.columns = [i if i == 'EID' else 'inv_begin_end_gap_' + i for i in inv_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(inv_num, inv_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_begin_end_gap, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lawsuit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    law_num = df.groupby('EID').size().reset_index()\n",
    "    law_num.columns = ['EID', 'law_count']\n",
    "    \n",
    "    # bad\n",
    "    df['lawdate'] = df['LAWDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    law_date = df.groupby('EID')['lawdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    law_date.columns = [i if i == 'EID' else 'law_date_' + i for i in law_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    law_amout = df.groupby('EID')['LAWAMOUNT'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    law_amout.columns = [i if i == 'EID' else 'law_amout_' + i for i in law_amout.columns]\n",
    "    \n",
    "    mydf = pd.merge(law_num, law_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, law_amout, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_project_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    pro_num = df.groupby('EID').size().reset_index()\n",
    "    pro_num.columns = ['EID', 'pro_count']\n",
    "    \n",
    "    df['djdate'] = df['DJDATE'].apply(translate_date)\n",
    "    pro_date = df.groupby('EID')['djdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    pro_date.columns = [i if i == 'EID' else 'pro_date_' + i for i in pro_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    pro_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    pro_home.columns = ['EID', 'pro_nothome_num', 'pro_home_num']\n",
    "    \n",
    "    mydf = pd.merge(pro_num, pro_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, pro_home, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qualification_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    qua_num = df.groupby('EID').size().reset_index()\n",
    "    qua_num.columns = ['EID', 'qua_count']\n",
    "    \n",
    "    # bad\n",
    "    qua_type = df.groupby(['EID', 'ADDTYPE']).size().unstack().reset_index()\n",
    "    qua_type.columns = [i if i == 'EID' else 'qua_type_' + str(i) for i in qua_type.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['begindate'] = df['BEGINDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '')).apply(translate_date)\n",
    "    qua_begindate = df.groupby('EID')['begindate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_begindate.columns = [i if i == 'EID' else 'qua_begindate_' + i for i in qua_begindate.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['expirydate'] = df['EXPIRYDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '') if not pd.isnull(x) else np.nan)\n",
    "    df['expirydate'] = (pd.to_datetime(df['expirydate']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    qua_expirydate = df.groupby('EID')['expirydate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_expirydate.columns = [i if i == 'EID' else 'qua_expirydate_' + i for i in qua_expirydate.columns]\n",
    "    \n",
    "    df['qua_begin_expiry_gap'] = df['expirydate'] - df['begindate']\n",
    "    qua_begin_expiry_gap = df.groupby('EID')['qua_begin_expiry_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    qua_begin_expiry_gap.columns = [i if i == 'EID' else 'qua_begin_expiry_gap_' + i for i in qua_begin_expiry_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(qua_num, qua_type, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begindate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_expirydate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begin_expiry_gap, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_breakfaith_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bre_num = df.groupby('EID').size().reset_index()\n",
    "    bre_num.columns = ['EID', 'bre_count']\n",
    "    \n",
    "    # bad\n",
    "    df['fbdate'] = df['FBDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    bre_date = df.groupby('EID')['fbdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_date.columns = [i if i == 'EID' else 'bre_date_' + i for i in bre_date.columns]\n",
    "    \n",
    "    df['sxenddate'] = (pd.to_datetime(df['SXENDDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    bre_enddate = df.groupby('EID')['sxenddate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_enddate.columns = [i if i == 'EID' else 'bre_enddate_' + i for i in bre_enddate.columns]\n",
    "    \n",
    "    df['bre_begin_end_gap'] = df['sxenddate'] - df['fbdate']\n",
    "    bre_begin_end_gap = df.groupby('EID')['bre_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    bre_begin_end_gap.columns = [i if i == 'EID' else 'bre_begin_end_gap_' + i for i in bre_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(bre_num, bre_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_enddate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_begin_end_gap, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "entbase_feat = get_entbase_feature(entbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:16: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "alter_feat = get_alter_feature(alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_feature = get_right_feature(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:22: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "recruit_feat = get_recruit_feature(recruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_feat = get_branch_feature(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invest_feat = get_invest_feature(invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lawsuit_feat = get_lawsuit_feature(lawsuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_feat = get_project_feature(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualification_feat = get_qualification_feature(qualification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breakfaith_feat = get_breakfaith_feature(breakfaith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, right_feature, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, recruit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, branch_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, invest_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, lawsuit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, project_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, qualification_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, breakfaith_feat, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['alt_count/rgyear'] = dataset['alt_count'] / dataset['RGYEAR']\n",
    "dataset['rig_count/rgyear'] = dataset['rig_count'] / dataset['RGYEAR']\n",
    "dataset['rec_count/rgyear'] = dataset['rec_count'] / dataset['RGYEAR']\n",
    "dataset['bra_count/rgyear'] = dataset['bra_count'] / dataset['RGYEAR']\n",
    "dataset['inv_count/rgyear'] = dataset['inv_count'] / dataset['RGYEAR']\n",
    "dataset['inved_num/rgyear'] = dataset['inved_num'] / dataset['RGYEAR']\n",
    "dataset['law_count/rgyear'] = dataset['law_count'] / dataset['RGYEAR']\n",
    "dataset['pro_count/rgyear'] = dataset['pro_count'] / dataset['RGYEAR']\n",
    "dataset['qua_count/rgyear'] = dataset['qua_count'] / dataset['RGYEAR']\n",
    "dataset['bre_count/rgyear'] = dataset['bre_count'] / dataset['RGYEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['alt_num(1year)/rgyear'] = dataset['alt_num(1year)'] / dataset['RGYEAR']\n",
    "dataset['alt_num(2year)/rgyear'] = dataset['alt_num(2year)'] / dataset['RGYEAR']\n",
    "dataset['alt_num(3year)/rgyear'] = dataset['alt_num(3year)'] / dataset['RGYEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MPNUM_CLASS'] = dataset['INUM'].apply(lambda x : x if x <= 4 else 5)\n",
    "dataset['FSTINUM_CLASS'] = dataset['FSTINUM'].apply(lambda x : x if x <= 6 else 7)\n",
    "dataset.fillna(value={'alt_count': 0, 'rig_count': 0}, inplace=True)\n",
    "for column in ['MPNUM', 'INUM', 'FINZB', 'FSTINUM', 'TZINUM', 'ENUM', 'ZCZB', 'allnum', 'RGYEAR', 'alt_count', 'rig_count']:\n",
    "    groupby_list = [['HY'], ['ETYPE'], ['HY', 'ETYPE'], ['HY', 'PROV'], ['ETYPE', 'PROV'], ['MPNUM_CLASS'], ['FSTINUM_CLASS']]\n",
    "#     groupby_list = [['HY'], ['ETYPE'], ['HY', 'PROV'], ['ETYPE', 'PROV'], ['MPNUM_CLASS'], ['FSTINUM_CLASS']]\n",
    "    for groupby in groupby_list:\n",
    "        if 'MPNUM_CLASS' in groupby and column == 'MPNUM':\n",
    "            continue\n",
    "        if 'FSTINUM_CLASS' in groupby and column == 'FSTINUM':\n",
    "            continue\n",
    "        groupby_keylist = []\n",
    "        for key in groupby:\n",
    "            groupby_keylist.append(dataset[key])\n",
    "        tmp = dataset[column].groupby(groupby_keylist).agg([sum, min, max, np.mean]).reset_index()\n",
    "        tmp = pd.merge(dataset, tmp, on=groupby, how='left')\n",
    "        dataset['ent_' + column.lower() + '-mean_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['mean']\n",
    "        dataset['ent_' + column.lower() + '-min_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['min']\n",
    "        dataset['ent_' + column.lower() + '-max_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['max']\n",
    "        dataset['ent_' + column.lower() + '/sum_gb_' + '_'.join(groupby).lower()] = dataset[column] / tmp['sum']\n",
    "dataset.drop(['MPNUM_CLASS', 'FSTINUM_CLASS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join(pd.get_dummies(dataset['PROV'], prefix='prov'))\n",
    "dataset = dataset.join(pd.get_dummies(dataset['HY'], prefix='hy').mul(dataset['ZCZB'], axis=0))\n",
    "dataset = dataset.join(pd.get_dummies(dataset['ETYPE'], prefix='etype'))\n",
    "dataset.drop(['PROV', 'HY', 'ETYPE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = pd.merge(train, dataset, on='EID', how='left')\n",
    "testset = pd.merge(test, dataset, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218264, 635) (218264L,) (218247, 635)\n"
     ]
    }
   ],
   "source": [
    "train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)\n",
    "train_label = trainset.TARGET.values\n",
    "test_feature = testset\n",
    "test_index = testset.EID.values\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:3: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# EID 前面的字母代表不同省份，已提供了 PROV 列，因此字母是冗余信息，直接舍弃\n",
    "train_feature['EID'] = train_feature['EID'].str.extract('(\\d+)').astype(int)\n",
    "test_feature['EID'] = test_feature['EID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.concat([train_feature, trainset.TARGET], axis=1).to_csv('../data/output/feat/train_xxy_local6864_online6923.csv', index=False)\n",
    "# test_feature.to_csv('../data/output/feat/test_xxy_local6864_online6923.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights ': 0,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 15,\n",
    "#     'gamma': 0.1,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.75,\n",
    "#     'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 42,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = xgb.cv(params, dtrain, num_round, nfold=folds, verbose_eval=10, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res), res.loc[len(res) - 1, 'test-auc-mean']\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=30)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, threshold, name):\n",
    "    result = pd.DataFrame({'EID': test_index, 'FORTARGET': 0, 'PROB': pred})\n",
    "    mask = result['PROB'] >= threshold\n",
    "    result.at[mask, 'FORTARGET'] = 1\n",
    "    # result['PROB'] = result['PROB'].apply(lambda x: round(x, 4))\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'EID', u'RGYEAR', u'ZCZB', u'MPNUM', u'INUM', u'ENUM', u'FINZB',\n",
      "       u'FSTINUM', u'TZINUM', u'allnum',\n",
      "       ...\n",
      "       u'etype_2', u'etype_3', u'etype_4', u'etype_5', u'etype_6', u'etype_7',\n",
      "       u'etype_8', u'etype_13', u'etype_16', u'etype_17'],\n",
      "      dtype='object', length=635)\n",
      "run cv: round: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "[0]\tcv-test-auc:0.644672+0.00293150834896\tcv-train-auc:0.6753368+0.00362655927292\n",
      "[10]\tcv-test-auc:0.6685698+0.00373066328687\tcv-train-auc:0.7094106+0.00111988920881\n",
      "[20]\tcv-test-auc:0.6705534+0.00354194108364\tcv-train-auc:0.7137188+0.000784800968399\n",
      "[30]\tcv-test-auc:0.671906+0.00366938016564\tcv-train-auc:0.7169934+0.000717434484814\n",
      "[40]\tcv-test-auc:0.6728856+0.00392799972505\tcv-train-auc:0.7192066+0.000802126324216\n",
      "[50]\tcv-test-auc:0.6733892+0.0041911327538\tcv-train-auc:0.721235+0.000624354706877\n",
      "[60]\tcv-test-auc:0.6739956+0.0042547186793\tcv-train-auc:0.723151+0.00066662883226\n",
      "[70]\tcv-test-auc:0.67442+0.00418770271151\tcv-train-auc:0.725056+0.00066300015083\n",
      "[80]\tcv-test-auc:0.6750882+0.00408306514276\tcv-train-auc:0.7267036+0.00071948414854\n",
      "[90]\tcv-test-auc:0.675577+0.00399252276136\tcv-train-auc:0.728558+0.000777215028161\n",
      "[100]\tcv-test-auc:0.6761088+0.00406188849675\tcv-train-auc:0.7302376+0.000764088110102\n",
      "[110]\tcv-test-auc:0.6766406+0.00390117657124\tcv-train-auc:0.731586+0.000678637458442\n",
      "[120]\tcv-test-auc:0.6770966+0.00376740768168\tcv-train-auc:0.7330964+0.000701636544088\n",
      "[130]\tcv-test-auc:0.6775774+0.00373761392335\tcv-train-auc:0.7344208+0.000729654000195\n",
      "[140]\tcv-test-auc:0.6780544+0.00385112464613\tcv-train-auc:0.7359112+0.000772903460983\n",
      "[150]\tcv-test-auc:0.6785762+0.00380008880949\tcv-train-auc:0.7373464+0.000789102680264\n",
      "[160]\tcv-test-auc:0.679006+0.00376769515752\tcv-train-auc:0.7387038+0.000825822111595\n",
      "[170]\tcv-test-auc:0.6794904+0.00367944477333\tcv-train-auc:0.7401704+0.00080409317868\n",
      "[180]\tcv-test-auc:0.679961+0.00361291129147\tcv-train-auc:0.741611+0.000667917659596\n",
      "[190]\tcv-test-auc:0.6802082+0.00360806559807\tcv-train-auc:0.7427696+0.000630444795363\n",
      "[200]\tcv-test-auc:0.6806414+0.00372786585596\tcv-train-auc:0.7439214+0.000619659454862\n",
      "[210]\tcv-test-auc:0.6810612+0.00372390888181\tcv-train-auc:0.7451536+0.000602512605677\n",
      "[220]\tcv-test-auc:0.6814624+0.00370914279046\tcv-train-auc:0.7462684+0.000528537075332\n",
      "[230]\tcv-test-auc:0.6818518+0.00364156331264\tcv-train-auc:0.74728+0.000576440803552\n",
      "[240]\tcv-test-auc:0.682105+0.0036307387678\tcv-train-auc:0.7483012+0.000586239677948\n",
      "[250]\tcv-test-auc:0.6823916+0.00363337813061\tcv-train-auc:0.7492008+0.00065236075909\n",
      "[260]\tcv-test-auc:0.6827308+0.00366850445822\tcv-train-auc:0.7501348+0.000689508346577\n",
      "[270]\tcv-test-auc:0.6830484+0.00364030337197\tcv-train-auc:0.7508766+0.000694234427265\n",
      "[280]\tcv-test-auc:0.6833156+0.00361453795664\tcv-train-auc:0.7516024+0.000713583800265\n",
      "[290]\tcv-test-auc:0.6835868+0.00362330972455\tcv-train-auc:0.752429+0.00074340971207\n",
      "[300]\tcv-test-auc:0.6838406+0.00358929567464\tcv-train-auc:0.7531942+0.000712875977993\n",
      "[310]\tcv-test-auc:0.684032+0.00356551325899\tcv-train-auc:0.753877+0.000711324679735\n",
      "[320]\tcv-test-auc:0.6842652+0.00356307397622\tcv-train-auc:0.7545512+0.000754940368506\n",
      "[330]\tcv-test-auc:0.6844934+0.00359476717466\tcv-train-auc:0.7551094+0.000749879083586\n",
      "[340]\tcv-test-auc:0.6846472+0.00359862481512\tcv-train-auc:0.7556938+0.000746860736684\n",
      "[350]\tcv-test-auc:0.6848464+0.00360710873692\tcv-train-auc:0.7563794+0.000857897103387\n",
      "[360]\tcv-test-auc:0.6850306+0.00358883973451\tcv-train-auc:0.7569936+0.000910057272923\n",
      "[370]\tcv-test-auc:0.6851996+0.00361591806323\tcv-train-auc:0.7575822+0.000919451445156\n",
      "[380]\tcv-test-auc:0.6853532+0.00362990723848\tcv-train-auc:0.758137+0.000851584170825\n",
      "[390]\tcv-test-auc:0.6854808+0.00364485859259\tcv-train-auc:0.758685+0.000845250968648\n",
      "[400]\tcv-test-auc:0.6856124+0.00366241377236\tcv-train-auc:0.7592238+0.000829466913144\n",
      "[410]\tcv-test-auc:0.6857872+0.00365224705627\tcv-train-auc:0.7597648+0.000782803525797\n",
      "[420]\tcv-test-auc:0.685899+0.00363487028654\tcv-train-auc:0.7602802+0.000791360448847\n",
      "[430]\tcv-test-auc:0.6860426+0.00363828138549\tcv-train-auc:0.760777+0.000812706343029\n",
      "[440]\tcv-test-auc:0.686168+0.00362420683736\tcv-train-auc:0.7612958+0.000764876565205\n",
      "[450]\tcv-test-auc:0.6862822+0.00361302650973\tcv-train-auc:0.7617386+0.000839453774785\n",
      "[460]\tcv-test-auc:0.6864334+0.00363676056952\tcv-train-auc:0.762281+0.000878383287637\n",
      "[470]\tcv-test-auc:0.6865422+0.00363038473994\tcv-train-auc:0.7627266+0.000850296089606\n",
      "[480]\tcv-test-auc:0.6866424+0.00364704590594\tcv-train-auc:0.7632422+0.000843384941767\n",
      "[490]\tcv-test-auc:0.6867198+0.00366526945258\tcv-train-auc:0.7638284+0.000829449600639\n",
      "[500]\tcv-test-auc:0.6868344+0.00363788694712\tcv-train-auc:0.7643642+0.00082097756364\n",
      "[510]\tcv-test-auc:0.6869182+0.00367637524744\tcv-train-auc:0.7649416+0.00083586568299\n",
      "[520]\tcv-test-auc:0.6870196+0.00365871078934\tcv-train-auc:0.7655544+0.000779267758861\n",
      "[530]\tcv-test-auc:0.6871226+0.00368204967919\tcv-train-auc:0.7660052+0.000786159118754\n",
      "[540]\tcv-test-auc:0.6872246+0.00364262567937\tcv-train-auc:0.766556+0.000799851236168\n",
      "[550]\tcv-test-auc:0.687321+0.00368001934778\tcv-train-auc:0.7670958+0.000831820990358\n",
      "[560]\tcv-test-auc:0.6873982+0.0037046745822\tcv-train-auc:0.7675902+0.000806807387175\n",
      "[570]\tcv-test-auc:0.687486+0.00368952295019\tcv-train-auc:0.768117+0.000848417821595\n",
      "[580]\tcv-test-auc:0.6875622+0.00367104603077\tcv-train-auc:0.7686238+0.000857601632461\n",
      "[590]\tcv-test-auc:0.6876482+0.00365785939588\tcv-train-auc:0.769034+0.000849488552012\n",
      "[600]\tcv-test-auc:0.6877314+0.0036726381036\tcv-train-auc:0.7695542+0.000855027812413\n",
      "[610]\tcv-test-auc:0.6878068+0.00365420264353\tcv-train-auc:0.7700908+0.000843691507602\n",
      "[620]\tcv-test-auc:0.68787+0.00367127274933\tcv-train-auc:0.7705626+0.000863760753913\n",
      "[630]\tcv-test-auc:0.6879278+0.00367904353875\tcv-train-auc:0.7710286+0.000803948406305\n",
      "[640]\tcv-test-auc:0.6880008+0.0036698023053\tcv-train-auc:0.771486+0.000811781990438\n",
      "[650]\tcv-test-auc:0.6880674+0.00366722039698\tcv-train-auc:0.772003+0.000811371185093\n",
      "[660]\tcv-test-auc:0.6881296+0.00367950668433\tcv-train-auc:0.772547+0.000894042280879\n",
      "[670]\tcv-test-auc:0.6881952+0.0036478039092\tcv-train-auc:0.7730826+0.00086794253266\n",
      "[680]\tcv-test-auc:0.6882372+0.00361403001648\tcv-train-auc:0.7735744+0.000877245370464\n",
      "[690]\tcv-test-auc:0.6883108+0.00366463430099\tcv-train-auc:0.7740708+0.000933932417255\n",
      "[700]\tcv-test-auc:0.688372+0.00366521906576\tcv-train-auc:0.7745822+0.00100669208798\n",
      "[710]\tcv-test-auc:0.6884032+0.00368240325874\tcv-train-auc:0.7750798+0.0010356681708\n",
      "[720]\tcv-test-auc:0.6884508+0.00368513111843\tcv-train-auc:0.775589+0.00103921258653\n",
      "[730]\tcv-test-auc:0.6884856+0.00369915593616\tcv-train-auc:0.7761378+0.00111444971174\n",
      "[740]\tcv-test-auc:0.6885098+0.00372768525495\tcv-train-auc:0.776651+0.00116983041506\n",
      "[750]\tcv-test-auc:0.6885548+0.00372527273632\tcv-train-auc:0.7770946+0.00112681739426\n",
      "[760]\tcv-test-auc:0.6885904+0.00373773934886\tcv-train-auc:0.7775844+0.00113080583656\n",
      "[770]\tcv-test-auc:0.6886134+0.00372526200958\tcv-train-auc:0.7780822+0.00115619105688\n",
      "[780]\tcv-test-auc:0.688656+0.00371467365996\tcv-train-auc:0.7786304+0.00110486172891\n",
      "[790]\tcv-test-auc:0.6886788+0.00369556379461\tcv-train-auc:0.7790976+0.00113380855527\n",
      "[800]\tcv-test-auc:0.688701+0.00369227577518\tcv-train-auc:0.7795648+0.00113708705032\n",
      "[810]\tcv-test-auc:0.6887392+0.00370810881178\tcv-train-auc:0.7800888+0.00113279043075\n",
      "[820]\tcv-test-auc:0.6887684+0.00371226747959\tcv-train-auc:0.7806566+0.00120396172697\n",
      "[830]\tcv-test-auc:0.6887794+0.00369872997663\tcv-train-auc:0.781077+0.00125402743192\n",
      "[840]\tcv-test-auc:0.6888022+0.00368737182286\tcv-train-auc:0.781534+0.00133309084462\n",
      "[850]\tcv-test-auc:0.6887904+0.00368710057362\tcv-train-auc:0.7820546+0.00134289204332\n",
      "[860]\tcv-test-auc:0.6888106+0.00365990205333\tcv-train-auc:0.7824822+0.00125722001257\n",
      "[870]\tcv-test-auc:0.6888176+0.00363378979029\tcv-train-auc:0.7829658+0.00126164042421\n",
      "[880]\tcv-test-auc:0.6888526+0.00362982867915\tcv-train-auc:0.7834542+0.00121673701349\n",
      "[890]\tcv-test-auc:0.6888528+0.0036202987943\tcv-train-auc:0.783945+0.00116636889533\n",
      "[900]\tcv-test-auc:0.6888666+0.00362877613528\tcv-train-auc:0.7843962+0.00126837587489\n",
      "[910]\tcv-test-auc:0.6888604+0.00364418778879\tcv-train-auc:0.7849522+0.00127016115513\n",
      "[920]\tcv-test-auc:0.6888828+0.00363849916312\tcv-train-auc:0.7854704+0.00133139920385\n",
      "[930]\tcv-test-auc:0.688907+0.00363246406727\tcv-train-auc:0.785879+0.00130261352672\n",
      "[940]\tcv-test-auc:0.6889002+0.00365020664621\tcv-train-auc:0.7863216+0.00132572313852\n",
      "[950]\tcv-test-auc:0.68892+0.00364979550112\tcv-train-auc:0.7867468+0.00134284077984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[960]\tcv-test-auc:0.6889212+0.0036602170646\tcv-train-auc:0.787166+0.00137513344807\n",
      "[970]\tcv-test-auc:0.688924+0.00364885121648\tcv-train-auc:0.787563+0.00137992999822\n",
      "[980]\tcv-test-auc:0.6889442+0.00365119467572\tcv-train-auc:0.7880114+0.00132900272385\n",
      "[990]\tcv-test-auc:0.6889414+0.00363465522987\tcv-train-auc:0.7884854+0.0012883334351\n",
      "[1000]\tcv-test-auc:0.6889602+0.00364043724846\tcv-train-auc:0.7890264+0.00129656416733\n",
      "[1010]\tcv-test-auc:0.6889812+0.00364076977575\tcv-train-auc:0.78948+0.0013160711227\n",
      "[1020]\tcv-test-auc:0.6889904+0.00362850110101\tcv-train-auc:0.7899028+0.00139601595979\n",
      "[1030]\tcv-test-auc:0.68901+0.00363425783345\tcv-train-auc:0.790419+0.0014608264784\n",
      "[1040]\tcv-test-auc:0.6890302+0.00364768323186\tcv-train-auc:0.7909452+0.00142802540594\n",
      "[1050]\tcv-test-auc:0.6890328+0.00364790687381\tcv-train-auc:0.791421+0.00140838687867\n",
      "[1060]\tcv-test-auc:0.6890108+0.00365550272329\tcv-train-auc:0.7918746+0.00139337627366\n",
      "[1070]\tcv-test-auc:0.6890234+0.0036549160647\tcv-train-auc:0.792326+0.00145344446058\n",
      "[1080]\tcv-test-auc:0.689034+0.00367203447696\tcv-train-auc:0.7927646+0.00144637679738\n",
      "[1090]\tcv-test-auc:0.6890402+0.00367305994506\tcv-train-auc:0.7932432+0.00147276154214\n",
      "[1100]\tcv-test-auc:0.689038+0.00368679128783\tcv-train-auc:0.7936594+0.0014460437891\n",
      "[1110]\tcv-test-auc:0.68904+0.00367425949002\tcv-train-auc:0.794101+0.00146043787954\n",
      "[1120]\tcv-test-auc:0.6890412+0.00365405366135\tcv-train-auc:0.7945792+0.00144054759033\n",
      "[1130]\tcv-test-auc:0.6890624+0.00366163933778\tcv-train-auc:0.794987+0.00140853185977\n",
      "[1140]\tcv-test-auc:0.6890656+0.00367099183328\tcv-train-auc:0.7953818+0.00140134412619\n",
      "[1150]\tcv-test-auc:0.689067+0.00366110731883\tcv-train-auc:0.7957922+0.0014367484679\n",
      "[1160]\tcv-test-auc:0.6890652+0.0036500100493\tcv-train-auc:0.7962056+0.00145758863881\n",
      "[1170]\tcv-test-auc:0.6890488+0.00365762176284\tcv-train-auc:0.7966772+0.00149844498064\n",
      "[1180]\tcv-test-auc:0.6890408+0.00364423168308\tcv-train-auc:0.7971252+0.00146725954078\n",
      "[1190]\tcv-test-auc:0.6890396+0.00362234982297\tcv-train-auc:0.7976386+0.00147321425461\n",
      "[1200]\tcv-test-auc:0.6890242+0.00361195450691\tcv-train-auc:0.7980526+0.00149110302796\n",
      "[1210]\tcv-test-auc:0.6890228+0.00361897297033\tcv-train-auc:0.798482+0.0015051249782\n",
      "[1220]\tcv-test-auc:0.6890072+0.00363051486156\tcv-train-auc:0.7989146+0.00155868926987\n",
      "[1230]\tcv-test-auc:0.689029+0.00363977488315\tcv-train-auc:0.7993192+0.00154986081956\n",
      "[1240]\tcv-test-auc:0.6890082+0.00364630878561\tcv-train-auc:0.7997006+0.00150377785593\n",
      "[1250]\tcv-test-auc:0.6890282+0.00362655050427\tcv-train-auc:0.8001182+0.00142181284282\n",
      "Stopping. Best iteration:\n",
      "[1151] cv-mean:0.6890708\tcv-std:0.00366687476743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 8450.58828464 s\n"
     ]
    }
   ],
   "source": [
    "iterations, best_score = xgb_cv(train_feature, train_label, params, config['folds'], config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.670027\n",
      "[30]\ttrain-auc:0.713255\n",
      "[60]\ttrain-auc:0.717691\n",
      "[90]\ttrain-auc:0.722551\n",
      "[120]\ttrain-auc:0.727542\n",
      "[150]\ttrain-auc:0.731677\n",
      "[180]\ttrain-auc:0.735727\n",
      "[210]\ttrain-auc:0.739333\n",
      "[240]\ttrain-auc:0.742395\n"
     ]
    }
   ],
   "source": [
    "model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(model.get_fscore().items(), columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "importance.to_csv('../data/output/feat_imp/importance-1206-%f(r%d).csv' % (best_score, iterations), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, 0.18, '1206-xgb-%f(r%d)' % (best_score, iterations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
