{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/input/train.csv')\n",
    "entbase = pd.read_csv('../data/input/1entbase.csv')\n",
    "alter = pd.read_csv('../data/input/2alter.csv')\n",
    "branch = pd.read_csv('../data/input/3branch.csv')\n",
    "invest = pd.read_csv('../data/input/4invest.csv')\n",
    "right = pd.read_csv('../data/input/5right.csv')\n",
    "project = pd.read_csv('../data/input/6project.csv')\n",
    "lawsuit = pd.read_csv('../data/input/7lawsuit.csv')\n",
    "breakfaith = pd.read_csv('../data/input/8breakfaith.csv')\n",
    "recruit = pd.read_csv('../data/input/9recruit.csv')\n",
    "qualification = pd.read_csv('../data/input/10qualification.csv', encoding='gbk')\n",
    "test = pd.read_csv('../data/input/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[-2:])\n",
    "    return (year - 2010) * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entbase_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mydf = df.fillna(value={'ZCZB': 0, 'MPNUM': 0, 'INUM': 0, 'ENUM': 0, 'FINZB': 0, 'FSTINUM': 0, 'TZINUM': 0})  # 未处理 HY；ZCZB 为 0 表示缺失或错误\n",
    "    \n",
    "    zczb_gb_prov = mydf.groupby('PROV')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_zczb/sum_gb_prov'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_prov'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_prov'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_prov'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    # bad\n",
    "    zczb_gb_rgyear = mydf.groupby('RGYEAR')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_zczb/sum_gb_rgyear'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_rgyear'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_rgyear'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_rgyear'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_hy = mydf.groupby('HY')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_zczb/sum_gb_hy'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_hy'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_hy'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_hy'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    zczb_gb_etype = mydf.groupby('ETYPE')['ZCZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, zczb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_zczb/sum_gb_etype'] = mydf['ZCZB'] / tmp['sum']\n",
    "    mydf['ent_zczb-min_gb_etype'] = mydf['ZCZB'] - tmp['min']\n",
    "    mydf['ent_zczb-max_gb_etype'] = mydf['ZCZB'] - tmp['max']\n",
    "    mydf['ent_zczb-mean_gb_etype'] = mydf['ZCZB'] - tmp['mean']\n",
    "    \n",
    "    ##### bad\n",
    "    mpnum_gb_prov = mydf.groupby('PROV')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_mpnum/sum_gb_prov'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_prov'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_prov'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_prov'] = mydf['MPNUM'] - tmp['median']\n",
    "    \n",
    "    mpnum_gb_rgyear = mydf.groupby('RGYEAR')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_mpnum/sum_gb_rgyear'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_rgyear'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_rgyear'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_rgyear'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_hy = mydf.groupby('HY')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_mpnum/sum_gb_hy'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_hy'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_hy'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_hy'] = mydf['MPNUM'] - tmp['median']\n",
    "\n",
    "    mpnum_gb_etype = mydf.groupby('ETYPE')['MPNUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, mpnum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_mpnum/sum_gb_etype'] = mydf['MPNUM'] / tmp['sum']\n",
    "    mydf['ent_mpnum-min_gb_etype'] = mydf['MPNUM'] - tmp['min']\n",
    "    mydf['ent_mpnum-max_gb_etype'] = mydf['MPNUM'] - tmp['max']\n",
    "    mydf['ent_mpnum-median_gb_etype'] = mydf['MPNUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    inum_gb_prov = mydf.groupby('PROV')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_inum/sum_gb_prov'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_prov'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_prov'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_prov'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_rgyear = mydf.groupby('RGYEAR')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_inum/sum_gb_rgyear'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_rgyear'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_rgyear'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_rgyear'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_hy = mydf.groupby('HY')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_inum/sum_gb_hy'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_hy'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_hy'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_hy'] = mydf['INUM'] - tmp['median']\n",
    "\n",
    "    inum_gb_etype = mydf.groupby('ETYPE')['INUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, inum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_inum/sum_gb_etype'] = mydf['INUM'] / tmp['sum']\n",
    "    mydf['ent_inum-min_gb_etype'] = mydf['INUM'] - tmp['min']\n",
    "    mydf['ent_inum-max_gb_etype'] = mydf['INUM'] - tmp['max']\n",
    "    mydf['ent_inum-median_gb_etype'] = mydf['INUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    enum_gb_prov = mydf.groupby('PROV')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_enum/sum_gb_prov'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_prov'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_prov'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_prov'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_rgyear = mydf.groupby('RGYEAR')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_enum/sum_gb_rgyear'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_rgyear'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_rgyear'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_rgyear'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_hy = mydf.groupby('HY')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_enum/sum_gb_hy'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_hy'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_hy'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_hy'] = mydf['ENUM'] - tmp['median']\n",
    "\n",
    "    enum_gb_etype = mydf.groupby('ETYPE')['ENUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, enum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_enum/sum_gb_etype'] = mydf['ENUM'] / tmp['sum']\n",
    "    mydf['ent_enum-min_gb_etype'] = mydf['ENUM'] - tmp['min']\n",
    "    mydf['ent_enum-max_gb_etype'] = mydf['ENUM'] - tmp['max']\n",
    "    mydf['ent_enum-median_gb_etype'] = mydf['ENUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    finzb_gb_prov = mydf.groupby('PROV')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_finzb/sum_gb_prov'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_prov'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_prov'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_prov'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_rgyear = mydf.groupby('RGYEAR')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_finzb/sum_gb_rgyear'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_rgyear'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_rgyear'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_rgyear'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_hy = mydf.groupby('HY')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_finzb/sum_gb_hy'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_hy'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_hy'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_hy'] = mydf['FINZB'] - tmp['mean']\n",
    "\n",
    "    finzb_gb_etype = mydf.groupby('ETYPE')['FINZB'].agg([sum, min, max, np.mean, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, finzb_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_finzb/sum_gb_etype'] = mydf['FINZB'] / tmp['sum']\n",
    "    mydf['ent_finzb-min_gb_etype'] = mydf['FINZB'] - tmp['min']\n",
    "    mydf['ent_finzb-max_gb_etype'] = mydf['FINZB'] - tmp['max']\n",
    "    mydf['ent_finzb-mean_gb_etype'] = mydf['FINZB'] - tmp['mean']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    fstinum_gb_prov = mydf.groupby('PROV')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_fstinum/sum_gb_prov'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_prov'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_prov'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_prov'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_rgyear = mydf.groupby('RGYEAR')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_fstinum/sum_gb_rgyear'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_rgyear'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_rgyear'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_rgyear'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_hy = mydf.groupby('HY')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_fstinum/sum_gb_hy'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_hy'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_hy'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_hy'] = mydf['FSTINUM'] - tmp['median']\n",
    "\n",
    "    fstinum_gb_etype = mydf.groupby('ETYPE')['FSTINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, fstinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_fstinum/sum_gb_etype'] = mydf['FSTINUM'] / tmp['sum']\n",
    "    mydf['ent_fstinum-min_gb_etype'] = mydf['FSTINUM'] - tmp['min']\n",
    "    mydf['ent_fstinum-max_gb_etype'] = mydf['FSTINUM'] - tmp['max']\n",
    "    mydf['ent_fstinum-median_gb_etype'] = mydf['FSTINUM'] - tmp['median']\n",
    "    #####\n",
    "    \n",
    "    ##### bad\n",
    "    tzinum_gb_prov = mydf.groupby('PROV')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_prov, how='left', on='PROV')\n",
    "    mydf['ent_tzinum/sum_gb_prov'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_prov'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_prov'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_prov'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_rgyear = mydf.groupby('RGYEAR')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_rgyear, how='left', on='RGYEAR')\n",
    "    mydf['ent_tzinum/sum_gb_rgyear'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_rgyear'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_rgyear'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_rgyear'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_hy = mydf.groupby('HY')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_hy, how='left', on='HY')\n",
    "    mydf['ent_tzinum/sum_gb_hy'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_hy'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_hy'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_hy'] = mydf['TZINUM'] - tmp['median']\n",
    "\n",
    "    tzinum_gb_etype = mydf.groupby('ETYPE')['TZINUM'].agg([sum, min, max, np.median, np.ptp]).reset_index()\n",
    "    tmp = pd.merge(mydf, tzinum_gb_etype, how='left', on='ETYPE')\n",
    "    mydf['ent_tzinum/sum_gb_etype'] = mydf['TZINUM'] / tmp['sum']\n",
    "    mydf['ent_tzinum-min_gb_etype'] = mydf['TZINUM'] - tmp['min']\n",
    "    mydf['ent_tzinum-max_gb_etype'] = mydf['TZINUM'] - tmp['max']\n",
    "    mydf['ent_tzinum-median_gb_etype'] = mydf['TZINUM'] - tmp['median']\n",
    "    #####\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_alter_feature(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    alt_no = df.groupby(['EID', 'ALTERNO']).size().reset_index()\n",
    "    alt_no = alt_no.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    alt_no.columns = ['EID', 'alt_count', 'alt_types_count']\n",
    "\n",
    "    alt_no_oh = df.groupby(['EID', 'ALTERNO']).size().unstack().reset_index()\n",
    "    alt_no_oh.columns = [i if i == 'EID' else 'alt_' + i for i in alt_no_oh.columns]\n",
    "\n",
    "    df['date'] = df['ALTDATE'].apply(translate_date)\n",
    "    date = df.groupby('EID')['date'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    date.columns = ['EID', 'alt_date_min', 'alt_date_max', 'alt_date_ptp', 'alt_date_std']\n",
    "\n",
    "    df['altbe'] = df['ALTBE'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    df['altaf'] = df['ALTAF'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    alt_be_af = df.groupby('EID')['altbe', 'altaf'].agg([min, max, np.mean]).reset_index()\n",
    "    alt_be_af.columns = ['EID', 'alt_be_min', 'alt_be_max', 'alt_be_mean', 'alt_af_min', 'alt_af_max', 'alt_af_mean']\n",
    "    \n",
    "#     tmp = df.groupby(df.columns.tolist(), as_index=False).size().reset_index()\n",
    "#     tmp = tmp.drop(['ALTERNO', 'ALTDATE', 'ALTBE', 'ALTAF', 'date', 'altbe', 'altaf'], axis=1)\n",
    "#     tmp.columns = ['EID', 'alt_dup_count']\n",
    "#     alt_dup_count = tmp.groupby(['EID', 'alt_dup_count']).size().unstack().reset_index()\n",
    "#     alt_dup_count.columns = [i if i == 'EID' else 'alt_dup_count_' + str(i) for i in alt_dup_count]\n",
    "    \n",
    "#     alt_dup = tmp[tmp['alt_dup_count'] > 1].groupby('EID')['alt_dup_count'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "#     alt_dup.columns = [i if i == 'EID' else 'alt_dup_' + i for i in alt_dup]\n",
    "\n",
    "    mydf = pd.merge(alt_no, alt_no_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af, how='left', on='EID')\n",
    "#     mydf = pd.merge(mydf, alt_dup_count, how='left', on='EID')\n",
    "#     mydf = pd.merge(mydf, alt_dup, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_right_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rig_type = df.groupby(['EID', 'RIGHTTYPE']).size().reset_index()\n",
    "    rig_type = rig_type.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rig_type.columns = ['EID', 'rig_count', 'rig_types_count']\n",
    "    \n",
    "    rig_type_oh_rate = df.groupby(['EID', 'RIGHTTYPE']).size().unstack().reset_index()\n",
    "    rig_type_oh_rate.iloc[:, 1:] = rig_type_oh_rate.iloc[:, 1:].div(rig_type['rig_count'], axis='index')\n",
    "    rig_type_oh_rate.columns = [i if i == 'EID' else 'rig_rate_' + str(i) for i in rig_type_oh_rate.columns]\n",
    "    \n",
    "    df['ask_month'] = (pd.to_datetime(df['ASKDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    ask_date = df.groupby('EID')['ask_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    ask_date.columns = ['EID', 'rig_askdate_max', 'rig_askdate_min', 'rig_askdate_ptp', 'rig_askdate_std']\n",
    "\n",
    "    df['get_month'] = (pd.to_datetime(df['FBDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    get_date = df.groupby('EID')['get_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    get_date.columns = ['EID', 'rig_getdate_max', 'rig_getdate_min', 'rig_getdate_ptp', 'rig_getdate_std']\n",
    "    \n",
    "    # bad\n",
    "    unget = df[df.FBDATE.isnull()]\n",
    "    unget = unget.groupby('EID').size().reset_index()\n",
    "    unget.columns = ['EID', 'rig_unget_num']\n",
    "    \n",
    "    right_1year = df[df['ASKDATE'] >= '2015-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_1year.columns = ['EID', 'ask_num(1year)']\n",
    "    right_2year = df[df['ASKDATE'] >= '2014-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_2year.columns = ['EID', 'ask_num(2year)']\n",
    "    right_5year = df[df['ASKDATE'] >= '2010-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_5year.columns = ['EID', 'ask_num(5year)']\n",
    "    right_end_1year = df[df['FBDATE'] >= '2015-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_1year.columns = ['EID', 'get_num(1year)']\n",
    "    right_end_2year = df[df['FBDATE'] >= '2014-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_2year.columns = ['EID', 'get_num(2year)']\n",
    "    right_end_5year = df[df['FBDATE'] >= '2010-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_5year.columns = ['EID', 'get_num(5year)']\n",
    "    \n",
    "    mydf = pd.merge(rig_type, rig_type_oh_rate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, get_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, unget, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_5year, how='left', on='EID')\n",
    "    \n",
    "    # bad\n",
    "    mydf['ask_rate(1year)'] = mydf['ask_num(1year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(2year)'] = mydf['ask_num(2year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(5year)'] = mydf['ask_num(5year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(1year)'] = mydf['get_num(1year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(2year)'] = mydf['get_num(2year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(5year)'] = mydf['get_num(5year)'] / mydf['rig_count']\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recruit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rec_wz = df.groupby(['EID', 'WZCODE']).size().reset_index()\n",
    "    rec_wz = rec_wz.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_wz.columns = ['EID', 'rec_wz_count', 'rec_wz_types_count']\n",
    "    \n",
    "    # bad\n",
    "    rec_wz_oh = df.groupby(['EID', 'WZCODE']).size().unstack().reset_index()\n",
    "    rec_wz_oh.columns = [i if i == 'EID' else 'rec_wz_' + i for i in rec_wz_oh.columns]\n",
    "    \n",
    "    # bad\n",
    "    rec_pos = df.groupby(['EID', 'POSCODE']).size().reset_index()\n",
    "    rec_pos = rec_pos.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_pos.columns = ['EID', 'rec_pos_count', 'rec_pos_types_count']\n",
    "    \n",
    "    df['recdate'] = (pd.to_datetime(df['RECDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    rec_date = df.groupby('EID')['recdate'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_date.columns = ['EID', 'rec_date_max', 'rec_date_min', 'rec_date_ptp', 'rec_date_std']\n",
    "    \n",
    "    # bad\n",
    "    df['pnum'] = df['PNUM'].str.extract('(\\d+)').fillna(1).astype(int)  # 若干=1\n",
    "    rec_num = df.groupby('EID')['pnum'].agg([sum, max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_num.columns = ['EID' if i == 'EID' else 'rec_num_' + i for i in rec_num.columns]\n",
    "    \n",
    "    mydf = pd.merge(rec_wz, rec_wz_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_pos, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branch_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bra_num = df.groupby('EID')['TYPECODE'].size().reset_index()\n",
    "    bra_num.columns = ['EID', 'bra_count']\n",
    "    \n",
    "    # bad\n",
    "    bra_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    bra_home.columns = ['EID', 'bra_nothome', 'bra_home']\n",
    "    \n",
    "    bra_year = df.groupby('EID')['B_REYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_year.columns = [i if i == 'EID' else 'bra_year_' + i for i in bra_year.columns]\n",
    "    \n",
    "    bra_endyear = df.groupby('EID')['B_ENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_endyear.columns = [i if i == 'EID' else 'bra_endyear_' + i for i in bra_endyear.columns]\n",
    "    \n",
    "    bra_end_num = df[~df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_end_num.columns = ['EID', 'bra_end_num']\n",
    "    bra_notend_num = df[df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_notend_num.columns = ['EID', 'bra_notend_num']\n",
    "    \n",
    "    mydf = pd.merge(bra_num, bra_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_end_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_notend_num, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_invest_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    inv_num = df.groupby('EID').size().reset_index()\n",
    "    inv_num.columns = ['EID', 'inv_count']\n",
    "    \n",
    "    # bad\n",
    "    inv_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inv_home.columns = ['EID', 'inv_nothome_num', 'inv_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inv_bl = df.groupby('EID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_bl.columns = [i if i == 'EID' else 'inv_bl_' + i for i in inv_bl.columns]\n",
    "    \n",
    "    inv_year = df.groupby('EID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_year.columns = [i if i == 'EID' else 'inv_year_' + i for i in inv_year.columns]\n",
    "    \n",
    "    # bad\n",
    "    inv_endyear = df.groupby('EID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_endyear.columns = [i if i == 'EID' else 'inv_endyear_' + i for i in inv_endyear.columns]\n",
    "    \n",
    "    # bad\n",
    "    inved_num = df.groupby('BTEID').size().reset_index()\n",
    "    inved_num.columns = ['EID', 'inved_num']\n",
    "    \n",
    "    inved_home = df.groupby(['BTEID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inved_home.columns = ['EID', 'inved_nothome_num', 'inved_home_num']\n",
    "    \n",
    "    # bad\n",
    "    inved_bl = df.groupby('BTEID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_bl.columns = ['EID' if i == 'BTEID' else 'inved_bl_' + i for i in inved_bl.columns]\n",
    "    \n",
    "    inved_year = df.groupby('BTEID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_year.columns = ['EID' if i == 'BTEID' else 'inved_year_' + i for i in inved_year.columns]\n",
    "    \n",
    "    inved_endyear = df.groupby('BTEID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_endyear.columns = ['EID' if i == 'BTEID' else 'inved_endyear_' + i for i in inved_endyear.columns]\n",
    "    \n",
    "    mydf = pd.merge(inv_num, inv_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_endyear, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lawsuit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    law_num = df.groupby('EID').size().reset_index()\n",
    "    law_num.columns = ['EID', 'law_count']\n",
    "    \n",
    "    # bad\n",
    "    df['lawdate'] = df['LAWDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    law_date = df.groupby('EID')['lawdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    law_date.columns = [i if i == 'EID' else 'law_date_' + i for i in law_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    law_amout = df.groupby('EID')['LAWAMOUNT'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    law_amout.columns = [i if i == 'EID' else 'law_amout_' + i for i in law_amout.columns]\n",
    "    \n",
    "    mydf = pd.merge(law_num, law_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, law_amout, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_project_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    pro_num = df.groupby('EID').size().reset_index()\n",
    "    pro_num.columns = ['EID', 'pro_count']\n",
    "    \n",
    "    df['djdate'] = df['DJDATE'].apply(translate_date)\n",
    "    pro_date = df.groupby('EID')['djdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    pro_date.columns = [i if i == 'EID' else 'pro_date_' + i for i in pro_date.columns]\n",
    "    \n",
    "    # bad\n",
    "    pro_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    pro_home.columns = ['EID', 'pro_nothome_num', 'pro_home_num']\n",
    "    \n",
    "    mydf = pd.merge(pro_num, pro_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, pro_home, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qualification_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    qua_num = df.groupby('EID').size().reset_index()\n",
    "    qua_num.columns = ['EID', 'qua_count']\n",
    "    \n",
    "    # bad\n",
    "    qua_type = df.groupby(['EID', 'ADDTYPE']).size().unstack().reset_index()\n",
    "    qua_type.columns = [i if i == 'EID' else 'qua_type_' + str(i) for i in qua_type.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['begindate'] = df['BEGINDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '')).apply(translate_date)\n",
    "    qua_begindate = df.groupby('EID')['begindate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_begindate.columns = [i if i == 'EID' else 'qua_begindate_' + i for i in qua_begindate.columns]\n",
    "    \n",
    "    # bad\n",
    "    df['expirydate'] = df['EXPIRYDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '') if not pd.isnull(x) else np.nan)\n",
    "    df['expirydate'] = (pd.to_datetime(df['expirydate']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    qua_expirydate = df.groupby('EID')['expirydate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_expirydate.columns = [i if i == 'EID' else 'qua_expirydate_' + i for i in qua_expirydate.columns]\n",
    "    \n",
    "    mydf = pd.merge(qua_num, qua_type, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begindate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_expirydate, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_breakfaith_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bre_num = df.groupby('EID').size().reset_index()\n",
    "    bre_num.columns = ['EID', 'bre_count']\n",
    "    \n",
    "    # bad\n",
    "    df['fbdate'] = df['FBDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    bre_date = df.groupby('EID')['fbdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_date.columns = [i if i == 'EID' else 'bre_date_' + i for i in bre_date.columns]\n",
    "    \n",
    "    df['sxenddate'] = (pd.to_datetime(df['SXENDDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    bre_enddate = df.groupby('EID')['sxenddate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_enddate.columns = [i if i == 'EID' else 'bre_enddate_' + i for i in bre_enddate.columns]\n",
    "    \n",
    "    mydf = pd.merge(bre_num, bre_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_enddate, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entbase_feat = get_entbase_feature(entbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_feat = get_alter_feature(alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_feature = get_right_feature(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruit_feat = get_recruit_feature(recruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_feat = get_branch_feature(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_feat = get_invest_feature(invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lawsuit_feat = get_lawsuit_feature(lawsuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_feat = get_project_feature(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualification_feat = get_qualification_feature(qualification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfaith_feat = get_breakfaith_feature(breakfaith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, right_feature, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, recruit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, branch_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, invest_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, lawsuit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, project_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, qualification_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, breakfaith_feat, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = pd.merge(train, dataset, on='EID', how='left')\n",
    "testset = pd.merge(test, dataset, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)\n",
    "train_label = trainset.TARGET.values\n",
    "test_feature = testset\n",
    "test_index = testset.EID.values\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EID 前面的字母代表不同省份，已提供了 PROV 列，因此字母是冗余信息，直接舍弃\n",
    "train_feature['EID'] = train_feature['EID'].str.extract('(\\d+)').astype(int)\n",
    "test_feature['EID'] = test_feature['EID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.concat([train_feature, trainset.TARGET], axis=1).to_csv('../data/output/feat/train_xxy_local6864_online6923.csv', index=False)\n",
    "# test_feature.to_csv('../data/output/feat/test_xxy_local6864_online6923.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 3\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "#     'objective': 'rank:pairwise',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights ': 0,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 10,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 20,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, params, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = xgb.cv(params, dtrain, num_round, verbose_eval=10, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res), res.loc[len(res) - 1, 'test-auc-mean']\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=50)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, threshold, name):\n",
    "    result = pd.DataFrame({'EID': test_index, 'FORTARGET': 0, 'PROB': pred})\n",
    "    mask = result['PROB'] >= threshold\n",
    "    result.at[mask, 'FORTARGET'] = 1\n",
    "    # result['PROB'] = result['PROB'].apply(lambda x: round(x, 4))\n",
    "    result.to_csv('../data/output/sub/' + name + '.csv', index=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'EID', u'PROV', u'RGYEAR', u'HY', u'ZCZB', u'ETYPE', u'MPNUM', u'INUM',\n",
      "       u'ENUM', u'FINZB',\n",
      "       ...\n",
      "       u'qua_expirydate_std', u'bre_count', u'bre_date_min', u'bre_date_max',\n",
      "       u'bre_date_ptp', u'bre_date_std', u'bre_enddate_min',\n",
      "       u'bre_enddate_max', u'bre_enddate_ptp', u'bre_enddate_std'],\n",
      "      dtype='object', length=278)\n",
      "run cv: round: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "[0]\tcv-test-auc:0.644919+0.000634175580314\tcv-train-auc:0.665825+0.00208746800375\n",
      "[10]\tcv-test-auc:0.665447666667+0.00230400945214\tcv-train-auc:0.700366+0.000704714126437\n",
      "[20]\tcv-test-auc:0.667808333333+0.0021882803497\tcv-train-auc:0.705103+0.000657828751779\n",
      "[30]\tcv-test-auc:0.669566+0.00227178798893\tcv-train-auc:0.707969333333+0.000916152219279\n",
      "[40]\tcv-test-auc:0.670667666667+0.00238192433876\tcv-train-auc:0.710656+0.000709859610533\n",
      "[50]\tcv-test-auc:0.671738333333+0.00243155099108\tcv-train-auc:0.712697666667+0.00101033899701\n",
      "[60]\tcv-test-auc:0.672302666667+0.0025573425443\tcv-train-auc:0.714347333333+0.000801544481333\n",
      "[70]\tcv-test-auc:0.672927666667+0.00238737922324\tcv-train-auc:0.716456333333+0.000977691953304\n",
      "[80]\tcv-test-auc:0.673349+0.00231417256631\tcv-train-auc:0.718008333333+0.000901623104308\n",
      "[90]\tcv-test-auc:0.673856333333+0.00221008390992\tcv-train-auc:0.719644333333+0.000908146953356\n",
      "[100]\tcv-test-auc:0.674279333333+0.00218005509614\tcv-train-auc:0.721259+0.00076210541703\n",
      "[110]\tcv-test-auc:0.674667666667+0.00220171847025\tcv-train-auc:0.722765+0.000766312381909\n",
      "[120]\tcv-test-auc:0.675210333333+0.00219049497197\tcv-train-auc:0.724237+0.0008919801941\n",
      "[130]\tcv-test-auc:0.675649+0.002240907108\tcv-train-auc:0.725452666667+0.000957317200421\n",
      "[140]\tcv-test-auc:0.676133333333+0.00228977179843\tcv-train-auc:0.727022+0.00105652859245\n",
      "[150]\tcv-test-auc:0.676586+0.00226648685561\tcv-train-auc:0.728487333333+0.00105193926736\n",
      "[160]\tcv-test-auc:0.676990333333+0.00232839863903\tcv-train-auc:0.729673+0.00111629386812\n",
      "[170]\tcv-test-auc:0.677397+0.00237779772058\tcv-train-auc:0.730834+0.00113216518229\n",
      "[180]\tcv-test-auc:0.677839666667+0.00239506051884\tcv-train-auc:0.732044666667+0.0012662496682\n",
      "[190]\tcv-test-auc:0.678263666667+0.00235918889075\tcv-train-auc:0.733256333333+0.00128037971798\n",
      "[200]\tcv-test-auc:0.678631666667+0.00227207193744\tcv-train-auc:0.734270666667+0.00135334556645\n",
      "[210]\tcv-test-auc:0.678979333333+0.00232359395956\tcv-train-auc:0.735451666667+0.00136355059394\n",
      "[220]\tcv-test-auc:0.679321666667+0.002282912954\tcv-train-auc:0.736640333333+0.00143472002689\n",
      "[230]\tcv-test-auc:0.679605666667+0.00224508965424\tcv-train-auc:0.737618333333+0.00144061013309\n",
      "[240]\tcv-test-auc:0.679986333333+0.00221755125207\tcv-train-auc:0.738591+0.00143428193416\n",
      "[250]\tcv-test-auc:0.680334333333+0.00221829098382\tcv-train-auc:0.739585333333+0.00130332404089\n",
      "[260]\tcv-test-auc:0.680612+0.00225383110873\tcv-train-auc:0.740553666667+0.00129877386621\n",
      "[270]\tcv-test-auc:0.680958+0.00225085953952\tcv-train-auc:0.741556333333+0.00136108347364\n",
      "[280]\tcv-test-auc:0.681198333333+0.00222584730434\tcv-train-auc:0.742376+0.00128178547347\n",
      "[290]\tcv-test-auc:0.681449+0.00221082624072\tcv-train-auc:0.743118666667+0.00133362171881\n",
      "[300]\tcv-test-auc:0.681721+0.00219760839702\tcv-train-auc:0.743873+0.00125691368041\n",
      "[310]\tcv-test-auc:0.681947+0.00221467213525\tcv-train-auc:0.744652666667+0.00121325466778\n",
      "[320]\tcv-test-auc:0.682157333333+0.00218439088891\tcv-train-auc:0.745309+0.00106814449709\n",
      "[330]\tcv-test-auc:0.682371+0.00211672309636\tcv-train-auc:0.745944333333+0.00109602808155\n",
      "[340]\tcv-test-auc:0.682572+0.00213583192223\tcv-train-auc:0.746654333333+0.00107899531458\n",
      "[350]\tcv-test-auc:0.682743+0.00215749916956\tcv-train-auc:0.747396333333+0.00100338936721\n",
      "[360]\tcv-test-auc:0.682903+0.00212926340941\tcv-train-auc:0.748117333333+0.000882822871375\n",
      "[370]\tcv-test-auc:0.683070333333+0.00214113837842\tcv-train-auc:0.748848333333+0.000984478655036\n",
      "[380]\tcv-test-auc:0.68327+0.00211403074717\tcv-train-auc:0.749474666667+0.00100687448186\n",
      "[390]\tcv-test-auc:0.683448666667+0.00209718769361\tcv-train-auc:0.750077333333+0.000983222705641\n",
      "[400]\tcv-test-auc:0.683565666667+0.00207822381428\tcv-train-auc:0.750702333333+0.00095501529947\n",
      "[410]\tcv-test-auc:0.683736+0.00207594364085\tcv-train-auc:0.751303+0.00102140132498\n",
      "[420]\tcv-test-auc:0.683818666667+0.00210566162735\tcv-train-auc:0.751819666667+0.00117086217046\n",
      "[430]\tcv-test-auc:0.683917666667+0.00209458115675\tcv-train-auc:0.752449333333+0.0011709723405\n",
      "[440]\tcv-test-auc:0.684032+0.00212919343101\tcv-train-auc:0.75297+0.0011168637637\n",
      "[450]\tcv-test-auc:0.684137666667+0.00213832400622\tcv-train-auc:0.753668666667+0.00110348850873\n",
      "[460]\tcv-test-auc:0.684232666667+0.00216010143795\tcv-train-auc:0.754243666667+0.00107374246861\n",
      "[470]\tcv-test-auc:0.684321333333+0.00217167237144\tcv-train-auc:0.754785333333+0.00113392660942\n",
      "[480]\tcv-test-auc:0.684384+0.00216491985995\tcv-train-auc:0.755317666667+0.00101586427352\n",
      "[490]\tcv-test-auc:0.684419+0.00215898417471\tcv-train-auc:0.755786333333+0.00106010858983\n",
      "[500]\tcv-test-auc:0.684550333333+0.00221230804566\tcv-train-auc:0.756386666667+0.00104987120903\n",
      "[510]\tcv-test-auc:0.684661+0.00223517292396\tcv-train-auc:0.75693+0.00109660050459\n",
      "[520]\tcv-test-auc:0.684762333333+0.00229068262509\tcv-train-auc:0.757508333333+0.00102211034412\n",
      "[530]\tcv-test-auc:0.684828333333+0.0022928120047\tcv-train-auc:0.757975333333+0.00095325174476\n",
      "[540]\tcv-test-auc:0.684910333333+0.00230277750168\tcv-train-auc:0.758462333333+0.000965678115224\n",
      "[550]\tcv-test-auc:0.684969666667+0.0023428554278\tcv-train-auc:0.758938666667+0.000885070744191\n",
      "[560]\tcv-test-auc:0.685044+0.00229435263201\tcv-train-auc:0.759468333333+0.000884784468419\n",
      "[570]\tcv-test-auc:0.685124333333+0.00229747779581\tcv-train-auc:0.759918333333+0.000886155115591\n",
      "[580]\tcv-test-auc:0.685179333333+0.002297548597\tcv-train-auc:0.760467666667+0.000913761943226\n",
      "[590]\tcv-test-auc:0.685273333333+0.0023225947176\tcv-train-auc:0.761068333333+0.000919028593437\n",
      "[600]\tcv-test-auc:0.68535+0.00231855831643\tcv-train-auc:0.761691666667+0.0009157548192\n",
      "[610]\tcv-test-auc:0.685384666667+0.00235414943357\tcv-train-auc:0.762239333333+0.000992084113817\n",
      "[620]\tcv-test-auc:0.685415333333+0.0023749274141\tcv-train-auc:0.762769666667+0.00103296767724\n",
      "[630]\tcv-test-auc:0.685450333333+0.00239237766435\tcv-train-auc:0.763302666667+0.00103943360645\n",
      "[640]\tcv-test-auc:0.685512666667+0.0024119787635\tcv-train-auc:0.763869333333+0.00102043857673\n",
      "[650]\tcv-test-auc:0.685555666667+0.00241883723213\tcv-train-auc:0.76428+0.00102119570439\n",
      "[660]\tcv-test-auc:0.685601333333+0.00238906987945\tcv-train-auc:0.764767666667+0.00100605411827\n",
      "[670]\tcv-test-auc:0.685659+0.00237937933083\tcv-train-auc:0.765328333333+0.00102152119682\n",
      "[680]\tcv-test-auc:0.685710333333+0.00238258813525\tcv-train-auc:0.76579+0.000974278536491\n",
      "[690]\tcv-test-auc:0.685728+0.00240238228987\tcv-train-auc:0.766339333333+0.00106391739445\n",
      "[700]\tcv-test-auc:0.685764666667+0.00240329140047\tcv-train-auc:0.766965666667+0.000996463524448\n",
      "[710]\tcv-test-auc:0.685796333333+0.0024190406271\tcv-train-auc:0.767509333333+0.00099017922059\n",
      "[720]\tcv-test-auc:0.685835666667+0.00242535500265\tcv-train-auc:0.767972666667+0.00101776367045\n",
      "[730]\tcv-test-auc:0.685847666667+0.0024465083055\tcv-train-auc:0.768484+0.00102649987823\n",
      "[740]\tcv-test-auc:0.685871333333+0.00244986589202\tcv-train-auc:0.768954333333+0.000958926598975\n",
      "[750]\tcv-test-auc:0.685920666667+0.00244530084493\tcv-train-auc:0.769495+0.000960338829095\n",
      "[760]\tcv-test-auc:0.685955333333+0.00244133164937\tcv-train-auc:0.770092+0.000933338452367\n",
      "[770]\tcv-test-auc:0.685968+0.00246490243215\tcv-train-auc:0.770653+0.00089166922118\n",
      "[780]\tcv-test-auc:0.685979+0.00248101793625\tcv-train-auc:0.771053+0.000925673808639\n",
      "[790]\tcv-test-auc:0.686003+0.0024684312157\tcv-train-auc:0.771554+0.000853942621023\n",
      "[800]\tcv-test-auc:0.686028333333+0.00247158253397\tcv-train-auc:0.772009666667+0.000828534583198\n",
      "[810]\tcv-test-auc:0.686066+0.00247625725643\tcv-train-auc:0.772547+0.000956879651088\n",
      "[820]\tcv-test-auc:0.686089+0.00249413886275\tcv-train-auc:0.773001333333+0.000982857851144\n",
      "[830]\tcv-test-auc:0.686138666667+0.00249901131561\tcv-train-auc:0.773506666667+0.0010643196053\n",
      "[840]\tcv-test-auc:0.686141666667+0.00250599099936\tcv-train-auc:0.773913+0.00100079068741\n",
      "[850]\tcv-test-auc:0.686153333333+0.00252460192682\tcv-train-auc:0.774434333333+0.000961113705841\n",
      "[860]\tcv-test-auc:0.686185333333+0.00254267973253\tcv-train-auc:0.774845+0.00103250084746\n",
      "[870]\tcv-test-auc:0.686194666667+0.00255457815609\tcv-train-auc:0.775503+0.000978849324462\n",
      "[880]\tcv-test-auc:0.686245666667+0.00256014509137\tcv-train-auc:0.776043666667+0.000954950725896\n",
      "[890]\tcv-test-auc:0.686278+0.00257532405728\tcv-train-auc:0.776500666667+0.0009849119532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[900]\tcv-test-auc:0.68629+0.00259868902847\tcv-train-auc:0.777034666667+0.00100137583132\n",
      "[910]\tcv-test-auc:0.686297+0.00260146471563\tcv-train-auc:0.777498333333+0.000981170842526\n",
      "[920]\tcv-test-auc:0.686317+0.00261833420327\tcv-train-auc:0.778052333333+0.00091893356065\n",
      "[930]\tcv-test-auc:0.68634+0.00261932828539\tcv-train-auc:0.778442+0.000912402323539\n",
      "[940]\tcv-test-auc:0.686331666667+0.00263006531393\tcv-train-auc:0.778805+0.000955281459396\n",
      "[950]\tcv-test-auc:0.686343333333+0.00265176486305\tcv-train-auc:0.779322666667+0.000994293830928\n",
      "[960]\tcv-test-auc:0.686353+0.00265873064951\tcv-train-auc:0.77971+0.000974442746736\n",
      "[970]\tcv-test-auc:0.686376333333+0.00266570120023\tcv-train-auc:0.780196333333+0.000969324277812\n",
      "[980]\tcv-test-auc:0.686397+0.00266256730744\tcv-train-auc:0.780577333333+0.00097007262042\n",
      "[990]\tcv-test-auc:0.686414333333+0.00264785477111\tcv-train-auc:0.781081666667+0.00105073191422\n",
      "[1000]\tcv-test-auc:0.686371+0.0026226148529\tcv-train-auc:0.781577666667+0.00100623467552\n",
      "[1010]\tcv-test-auc:0.686374666667+0.00262197906594\tcv-train-auc:0.781944333333+0.00100412228118\n",
      "[1020]\tcv-test-auc:0.686391333333+0.00260538062905\tcv-train-auc:0.782470666667+0.00102297648501\n",
      "[1030]\tcv-test-auc:0.686395333333+0.00260733124521\tcv-train-auc:0.782945666667+0.00104090259978\n",
      "[1040]\tcv-test-auc:0.686398+0.00260742874623\tcv-train-auc:0.783469666667+0.00097787331604\n",
      "[1050]\tcv-test-auc:0.686413333333+0.00260840964745\tcv-train-auc:0.783823666667+0.000946376363939\n",
      "[1060]\tcv-test-auc:0.686416333333+0.00258077382883\tcv-train-auc:0.784347+0.000985800182593\n",
      "[1070]\tcv-test-auc:0.686431666667+0.00257468448984\tcv-train-auc:0.784794333333+0.00100952276954\n",
      "[1080]\tcv-test-auc:0.686423333333+0.00258722262067\tcv-train-auc:0.785269333333+0.00102831782808\n",
      "[1090]\tcv-test-auc:0.68643+0.00257013397835\tcv-train-auc:0.785671+0.00100756042002\n",
      "[1100]\tcv-test-auc:0.686434333333+0.00256559614038\tcv-train-auc:0.786191666667+0.00107733818068\n",
      "[1110]\tcv-test-auc:0.686456+0.002554123855\tcv-train-auc:0.786634333333+0.00108367656101\n",
      "[1120]\tcv-test-auc:0.686473333333+0.00255885655887\tcv-train-auc:0.787072333333+0.0011289668827\n",
      "[1130]\tcv-test-auc:0.686472666667+0.0025330738025\tcv-train-auc:0.787493666667+0.00110465268549\n",
      "[1140]\tcv-test-auc:0.686465+0.00251937068862\tcv-train-auc:0.787941666667+0.00116429845353\n",
      "[1150]\tcv-test-auc:0.686464333333+0.00250493011657\tcv-train-auc:0.788330666667+0.00109222871028\n",
      "[1160]\tcv-test-auc:0.686475+0.00249213442655\tcv-train-auc:0.788782666667+0.001162448374\n",
      "[1170]\tcv-test-auc:0.686469666667+0.00248915947438\tcv-train-auc:0.789251666667+0.00127675796018\n",
      "[1180]\tcv-test-auc:0.686467333333+0.00249517405316\tcv-train-auc:0.789680666667+0.00128323376237\n",
      "[1190]\tcv-test-auc:0.686476333333+0.00250890750903\tcv-train-auc:0.790156666667+0.00135459743918\n",
      "[1200]\tcv-test-auc:0.686467666667+0.00251808476603\tcv-train-auc:0.790598666667+0.00134921269224\n",
      "[1210]\tcv-test-auc:0.686456333333+0.00252515218463\tcv-train-auc:0.791001333333+0.00133763580827\n",
      "[1220]\tcv-test-auc:0.686434+0.00250970330252\tcv-train-auc:0.791484333333+0.00128046797001\n",
      "[1230]\tcv-test-auc:0.686439666667+0.00249476656668\tcv-train-auc:0.791922+0.00126780939682\n",
      "[1240]\tcv-test-auc:0.686397666667+0.00249473530103\tcv-train-auc:0.792374333333+0.00126249814081\n",
      "[1250]\tcv-test-auc:0.686390666667+0.00247981939307\tcv-train-auc:0.792824666667+0.00135319235719\n",
      "[1260]\tcv-test-auc:0.686385666667+0.00248919268483\tcv-train-auc:0.793210666667+0.00141627453396\n",
      "[1270]\tcv-test-auc:0.686366333333+0.00249679478977\tcv-train-auc:0.793639666667+0.00145070496273\n",
      "[1280]\tcv-test-auc:0.686351666667+0.00249846170451\tcv-train-auc:0.794133333333+0.00144941191139\n",
      "[1290]\tcv-test-auc:0.686344666667+0.00250178207595\tcv-train-auc:0.794504333333+0.00145536578067\n",
      "Stopping. Best iteration:\n",
      "[1194] cv-mean:0.686484666667\tcv-std:0.0025070415943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 2009.27386004 s\n"
     ]
    }
   ],
   "source": [
    "iterations, best_score = xgb_cv(train_feature, train_label, params, config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, pred = xgb_predict(train_feature, train_label, test_feature, iterations+100, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(model.get_fscore().items(), columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "importance.to_csv('../data/output/feat_imp/importance-1202-%f(r%d+100).csv' % (best_score, iterations), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, 0.19, '1202-xgb-%f(r%d+100)' % (best_score, iterations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
